---
layout: single
title: "CH4 Classification : Alternative Techniques"
categories: coding
---



## Types of Classifiers

- 분류자를 구별하는 많은 방법이 있다.
- classifiers 구축을 위한 대안적인 기법에 대해 논의한다.
  - 가장 가까운 이웃 분류기, 베이지안 네트워크, 로지스틱 회귀, 인공 신경망, 앙상블 방법 등

#### Binary vs Multiclass

- 이진 분류기
  - 가능한 **두 레이블 중 하나**에 각 데이터 인스턴스 할당(예: +1 및 -1)
  - 긍정적인 클래스는 보통 부정적인 클래스에 비해 우리가 더 정확하게 예측하는 데 관심이 있는 카테고리를 가리킨다.
    -  (ex) 전자 메일 분류 → 스팸 메일 +1, 비 스팸 메일 -1
    - (ex) 암진단 → 악성코드+1, 양성코드-1
- 멀티클래스 분류기
  - 사용 가능한 레이블이 **두 개 이상**일 때 사용
  - 일부 분류기는 이진 클래스용으로만 설계되었다.
    - 로지스틱 회귀 분석
  - 이진 분류기를 다중 클래스 분류기로 변환하는 방법에 대해 논의할 것이다.

#### Deterministic vs Probabilistic

- 결정론적 분류자
  - 각 데이터 인스턴스에 **이산형 값** 레이블 생성
    - 스팸 +1, 비스팸 -1
- 확률론적 분류자
  - 인스턴스가 특정 클래스에 속할 가능성을 나타내는 0과 1 사이의 **연속 점수** 할당
    - (ex) (스팸, 비스팸) = (0.8, 0.2)
  - 모든 클래스의 확률 점수는 합계가 1이다.
  - 예
    - 네이브 베이즈 분류기, 베이지안 네트워크
  - 데이터 인스턴스는 일반적으로 확률 점수가 가장 높은 클래스에 할당된다.

#### Linear vs Nonlinear

- 선형분류기

  - **선형** 분리 초평면을 사용하여 여러 클래스에서 인스턴스 식별(예: 로지스틱 회귀 분석)

  - 유연성은 떨어지지만 모델 과적합에 덜 민감함

- 비선형 분류기
  - 보다 복잡하고 **비선형적**인 의사결정 표면 구성(예: 딥 러닝)
  - 더 유연하지만 모델 과적합에 더 민감함

#### Global vs Local

- 전역 분류기
  - 전체 데이터 세트에 **단일** 모형 적합
  - 모형이 매우 비선형적이지 않은 경우 이 단일 크기 전략은 효과적이지 **않을** 수 있다.
- 지역 분류기
  - 입력 공간을 더 작은 영역으로 분할하고 **각 영역**의 훈련 인스턴스에 개별 모델을 맞춥니다(예: k-가장 가까운 이웃 분류기).
  - 더 유연하지만 모델 과적합에 더 민감함

#### Generative vs Discriminative

- 생성 분류기
  - 각 클래스 레이블에 속하는 인스턴스를 생성하는 **근본적인 분포** 설명(예: 나이브 베이즈 분류기)
    - (ex) 전자 메일의 단어가 스팸을 생성할 확률
  - 데이터의 확률 분포
- 판별분류기
  - 모든 클래스 레이블(예: 의사 결정 트리, 신경망)의 분포를 명시적으로 설명하지 않고 클래스 레이블을 **직접** 예측한다.
  - 의사결정 경계





## Nearest Neighbor Classifiers

- Eager 학습자
  - 교육 데이터가 제공되는 즉시 모델 학습
  - 예) 결정 트리 분류기
- 게으른 학습자
  - 테스트 인스턴스를 분류하는 데 필요할 때까지 교육 데이터 모델링 프로세스 지연
  - (ex) 가장 가까운 이웃 분류기
- 가장 가까운 이웃 분류기의 기본 개념
  - 테스트 인스턴스가 주어지면 이와 **유사한** 모든 교육 인스턴스를 찾는다.
  - 이러한 인스턴스를 **가장 가까운 이웃**이라고 합니다.
  - 가장 가까운 이웃을 기준으로 인스턴스의 클래스 레이블 결정

#### k-Nearest Neighbor Classifiers

- 각 인스턴스(x, y)를 d차원 공간의 점으로 간주한다.
  - x =(x1, x2, ..., xd) : d속성 값의 집합
  - y : 인스턴스 클래스 레이블
- 새 인스턴스 x'이 주어지면 각 트레이닝 인스턴스 (x, y)까지의 거리 (즉 거리(x',x))를 계산한다.
  - 앞에서 설명한 근접 측정 중 하나에 따라
  - (ex) 유클리드 거리, 맨해튼 거리, 코사인 거리, …
- x'에 가장 가까운 k 트레이닝 인스턴스 찾기
  – 우리는 이런 사례들을 **k-가까운 이웃**이라고 부른다.
- k-근접 이웃의 클래스 레이블을 기준으로 x' 분류
  - 이웃에 둘 이상의 레이블이 있는 경우 가장 가까운 이웃의 **다수** 클래스에 x'를 할당한다.
  - tiebreak를 피하기 위해 보통 k를 홀수로 잡는다.

#### The Right Value for k

-  k가 너무 작을 경우 (모델 유연)
  - k-NN 분류기는 **노이즈**로 인해 과적합에 취약할 수 있다.
    - 교육 데이터의 매우 구체적인 사례에 의해 영향을 받기 때문
- k가 너무 큰 경우 (모델 둔감)
  - k-NN 분류기는 가장 가까운 이웃 목록에 이웃에서 **멀리 떨어진** 훈련 예가 포함되어 있기 때문에 테스트 인스턴스를 잘못 분류할 수 있다.

#### Two Voting Approaches

- 일단 새로운 인스턴스 x'의 k-근접 이웃을 얻으면, x'의 클래스 레이블을 결정하는 많은 접근법이 있을 수 있다.
- 접근방법 1: **다수결**
  - 모든 이웃이 분류에 **동일한** 영향을 미칩니다.
  - 이것은 알고리즘이 k의 선택에 민감하게 만든다.
- 접근 2: **거리 가중 투표**
  - **거리**에 따라 각 이웃의 가중치(xi, yi) : wi = 1/d(x', xi)^2
    - 멀수록 작은 값 (0에 가까움)
  - 결과적으로, 멀리 떨어진 훈련 예제는 더 약한 영향을 미친다.

#### Characteristics of k-NN Classifiers

1. 이들은 **인스턴스 기반 학습**의 예이다.
   - 글로벌 모델을 구축하지 않고 교육 인스턴스(예: **모델 없음**)를 사용합니다.
   - 인스턴스 간의 유사성(예: 의료 처방 간의 유사성)을 결정하기 위한 적절한 근접 측정 필요
2. 모델 빌드가 필요하지 않지만 테스트 인스턴스를 분류하는 데는 **상당한 비용**이 들 수 있습니다.
   - 왜냐하면 우리는 시험과 훈련의 유사성을 개별적으로 계산해야 하기 때문이다.
3. 그들은 **지역 정보**를 바탕으로 예측을 한다.
   - 반면, 의사 결정 트리는 전체 데이터에 적합한 글로벌 모델을 찾는다.
   - 결과적으로, k-NN 분류기는 잡음에 상당히 민감하다.
4. 그들은 **임의의** 형상의 결정 경계를 만든다.
   - 의사 결정 트리의 보다 유연하고 직선적인 의사 결정 경계
   - k가 증가하면 유연성이 저하.
5. 교육 세트와 테스트 세트 모두에서 결측값을 처리하는 데 **어려움이 있음**
   - 근접 계산은 일반적으로 모든 속성의 존재를 요구하기 때문에
6. 관련 없는 속성이 있으면 일반적으로 사용되는 근접 측정값이 **왜곡**될 수 있습니다.
   - 특히 관련 없는 속성의 수가 많을 때
   - 따라서, 무관하고 중복된 속성의 존재는 k-NN 분류기의 성능에 부정적인 영향을 미칠 수 있다.
7. 적절한 근접 측정 및 데이터 사전 처리 단계를 수행하지 **않으면** 잘못된 예측을 생성할 수 있습니다.
   - 키와 몸무게를 기준으로 사람을 분류한다고 가정
     - 높이 특성은 1.5m에서 1.85m까지 다양
     - 중량 특성: 40kg에서 120kg까지 다양
   - **정규화**하지 않으면, 근접한 측정값이 가중치에 의해 지배될 수 있다.





## Naive Bayes Classifier

#### Probabilistic Classification Models

- 클래스 레이블을 예측할 뿐만 아니라 모든 예측과 관련된 **신뢰도** 측도를 제공할 필요가 있다.
- 속성과 클래스 레이블 간의 관계를 나타내기 위해 **확률 이론**을 사용
  - 데이터의 **불확실성**을 정량화하는 체계적인 방법을 제공한다.
  - 예측의 신뢰도를 평가하기 위한 매력적인 프레임워크
- **네이브 베이즈 분류기**
  - 가장 단순하고 널리 사용되는 확률론적 분류기 중 하나

#### Bayes Theorem

- 결합확률 P(x, y)
  - x와 y를 함께 관측할 확률
- 조건부 확률 P(y|x)
  - x가 이미 관측된 다음 y가 관측될 확률
- 베이즈 정리
  - x가 발생한 후 y가 발생할 확률은 구하기 어려우나 y가 발생한 후 x가 발생하는 것은 알기 쉬움

#### Using Bayes Theorem for Classification

- 방정식을 사용해 **모든** 클래스에 대한 P(y|x1,...xd)를 계산한다.
- 그 후 P를 **최대화**하는 클래스를 선택한다.

#### Computing P(y|x1,...,xd)

- P(y|x1,...,xd) : 사후 확률
- P(y) = ny / n
  - n : 모든 트레이닝 인스턴스의 수
  - ny : y클래스에 속해있는 트레이닝 인스턴스의 수
- P(x1,...,xd | y) = nx|y/ny
  - nx|y : y클래스에 속해있는 값들 중 속성 값이 x =(x1,...xd)와 동일한 훈련 인스턴스의 수
- P(x1,...,xd) = nx/n
  - nx : 속성 값이 x와 동일한 교육 인스턴스 수

#### Problems in Computing P(y|x1,...,xd)

- 속성 수가 증가함에 따라 동일한 속성 값을 갖는 교육 인스턴스의 수가 **줄어든다**
- 이로인해 추정치가 굉장히 작아진다.

#### Naive Bayes Classifier

1. P(No), P(Yes)값 구하기
2. P(No|x), P(Yes\y)값 구하기
   - ex. 0.7 * 4/7 * 4/7 * 0.072 : P(y)에다가 y발생 시 x속성 확률3개
3. 2번 문항 값 2개 중 더 큰 값을 x의 클래스로 분류

#### Computing P(xi|y) for Continuous Attributes

- 전략 1 : 각 연속 속성을 범주형 속성으로 이산화하고 동일한 방법을 적용한다. (구간 나누기)
- 전략 2 : 특정 형태의 확률 분포를 가정하고 훈련 데이터를 사용하여 확률 분포를 추정한다. (확률밀도함수)
  - 전제는 데이터가 **정규분포**일 때만 가능하다.

#### Characteristics of Naive Bayes Classifiers

1. **확률론적** 분류 모델
   - 그들은 예측의 **불확실성**을 정량화한다.
2. **생성 분류** 모델입니다.
   - 모든 클래스에 속하는 데이터 인스턴스 **세대**의 기본 분포를 포착한다.
3. 네이브 베이즈 가정을 사용하면 고차원적 설정에서도 P(y|x1, …,xd)를 **쉽게** 계산할 수 있다.
   - 이것은 그들을 **간단**하고 **효과적**인 분류 기법으로 만든다.
   - 따라서 텍스트 분류와 같은 다양한 응용 문제에서 일반적으로 사용된다.
4. 소음점에 **강하다**.
   - 왜냐하면 이러한 점들은 확률 추정치에 유의하게 영향을 미칠 수 없기 때문 (KNN보다 장점)
5. 교육 및 테스트 세트의 **결측값을** 처리할 수 있다.
   - 교육 세트: P(xi|y)를 계산하는 동안 결측값을 무시
   - 테스트 세트: 비결측 속성 값만 사용
6. 무관한 속성에 **강하다**.
   - 만약 Xi가 무관한 속성이라면, P(xi|y)는 거의 모든 분류에 균일하게 분포되어 무시할 수 있는 영향을 준다.







## Bayesian (Belief) Networks

- 네이비 베이즈 가정은 많은 경우 **너무 경직**되어 있습니다.
  - (ex) 특히 속성이 서로 종속되어 있을 때
- 속성과 클래스 레이블 사이의 확률적 관계를 모델링하기 위한 **보다 유연한** 프레임워크
  - 확률 이론과 그래프 이론의 개념 구축
  - 나이브 베이즈 가정 완화(조건부 독립성)
- **방향성 비순환 그래프**(DAG)로 표현한다.
  - 각 **node**는 랜덤 변수(속성)를 나타냅니다.
  - 각 **edge**는 영향의 방향을 나타냅니다(즉, 종속성).
  - X에서 Y로 향하는 edge가 있는 경우
    - X를 Y의 **부모**라고 하고, Y를 X의 **자식**이라고 한다.
  - X에서 Z로 향하는 path가 있는 경우
    - X는 Z의 **조상**이고 Z는 X의 **후손**이다.

#### Naive Bayes vs Bayesian Network

- **Naive Bayes**
  - 대상 클래스 Y가 트리의 루트이다.
  - 모든 특성 Xi는 Y와 직접 연결
  - Y는 속성 생성의 배후에 있는 요인이다.
  - 특별한 유형의 베이지안 네트워크로 볼 수 있다.
  - X집합들은 서로 독립적이며 클래스 레이블(Y)에 의해 영향을 받는다.
- **Bayesian Network**
  - 대상 클래스 Y는 트리의 루트에 있을 필요는 없지만 그래프의 **아무 곳에나** 나타날 수 있다.
  - Y는 X3과 X4에 영향을 미치며, X1과 X2의 영향을 받는다.
  - 확률론적 관계보다 일반적인 구조를 제공한다.
  - 나이브 베이즈보다는 자유로운 형태지만 방향성은 사람이 그려줘야 함

#### Joint Probability

- P(Xi | X1, …, Xi-1)를 계산하는 것은 **parents(Xi)**만 고려해도 충분하다.
- 각 P(Xi|parents(Xi))의 값은 **확률표**를 사용하여 구할 수 있다.

#### Inference

- 기본 전략(네이브 베이즈와 동일)
  - 방정식을 사용하여 모든 클래스에 대해 P(y|x)를 계산한다.
  - 그런 다음 P(y|x)를 최대화하는 클래스를 선택
- 아직 측정되지 않은 값 U = {U1,...,Uk}
  - 우리는 관찰되지 않은 속성 U를 공동 확률에서 배제한다.
  - Marginalization : 값이 주어지지 않았더라도 가능한 값을 다 넣어서 배제시키는 방법

#### Learning Model Parameters

- 지금까지, 우리는 다음이 **이미 알려져** 있다고 가정했다.
  - 베이지안 네트워크의 **토폴로지**
  - 모든 노드에 대한 확률 테이블의 **값**
- 교육 데이터에서 베이지안 네트워크의 토폴로지 및 확률 테이블 값을 어떻게 **학습**할 수 있는가?
- 사례 1: 네트워크의 토폴로지를 알고 있으며 확률표만 계산하면 된다.
  - 우리는 P(Xi|parents(Xi))에 대한 확률표를 **쉽게** 계산할 수 있다.
  - 우리는 Xi의 모든 가치와 parents(Xi)의 모든 값의 조합에 대한 훈련 인스턴스의 비율을 계산한다.
- 사례 2: 네트워크의 토폴로지를 알 수 **없음**
  - 베이지안 네트워크의 구조를 배우는 것은 확률표를 배우는 것보다 **훨씬 더 어려운 작업**이다.
  - 최적의 그래프 구조를 찾기 위한 몇 가지 점수 매기기 접근법이 있다.
    - 그러나 그래프가 크면 계산적으로 불가능한 경우가 많다.
  - 따라서 일반적인 접근법은 도메인 전문가의 지식을 사용하는 것이다.

#### Characteristics of Bayesian Networks

1. 속성과 클래스 레이블 사이의 확률적 관계를 나타내기 위한 **강력한** 접근 방식을 제공한다.
   - 변수 간의 **복잡한** 형태의 종속성을 포착할 수 있다.
2. 조건부 독립성에 대한 나이브 베이즈 가정을 사용하지 **않는다.**
   - 따라서 관련되거나 중복되는 속성의 존재를 쉽게 처리할 수 있다.
3. 네이브 베이즈 분류기와 유사하게, 그들은 또한 훈련 데이터의 **노이즈**와 **무관한 속성**에 매우 강하다.
   - 또한 테스트뿐만 아니라 교육 중에 **누락된 값**도 처리할 수 있다.
4. 베이지안 네트워크의 구조를 배우는 것은 **어려운** 작업이다.
   - 전문가의 도움이 필요한 경우가 많습니다.
   - 그러나 일단 구조가 결정되면, 확률론적 표를 배우는 것은 매우 간단할 수 있다.
5. 복잡한 형태의 관계를 표현하는 추가적인 능력으로 인해 베이지안 네트워크는 네이비 베이즈 분류기에 비해 **오버피팅에 더 취약**하다.
   - 더욱이, 베이지안 네트워크는 일반적으로 네이비 베이즈 분류기보다 확률표를 효과적으로 학습하기 위해 더 많은 훈련 인스턴스를 필요로 한다. (모든 부모를 고려해야 함)





## Logistic Regression

- 타겟 속성은 y{0,1}이다
  - 1: 긍정 클래스 (ex. 악성, 스팸, 사기)
  - 0: 부정 클래스 (ex. 양성, 비스팸, 일반)

#### Logistic Function

- 분류 결정
  - Q(z) = 1 / 1+ e^-z
  - if Q(z) >= 0.5  <=> if z >= 0  >>> y = 1
  - if Q(z) < 0.5  <=> if z < 0  >>> y = 0

#### Cost Function

- h(xi)와 yi 사이의 차이 값(error)를 D.train에 속해있는 **모든** (xi, yi) 에 대하여 구한다. (파라미터를 w0, w1, ... , wn으로 설정하면서)
  - 다 구해서 더한 값이 J(w)임 : Cost Function
- 이러한 토탈 에러(cost function)을 가장 낮춰주는 w값은 뭘까요?

#### Minimizing J(w)

- 로그함수로 쓴 거 미분 방정식 너무 복잡해서 다른 거 씀

#### Gradient Descent Intuition

- 알파 : learning rate(>0)
- J'(w) > 0 일때는 w감소해라
- J'(w) < 0 일때는 w증가해라

#### Learning Rate alpha

- 알파가 너무 작으면
  - 너무 느릴 수 있다.
- 알파가 너무 크면
  - 최소값을 지나칠 수 있음
  - 수렴에 실패하거나 심지어는 갈라질 수도 있다.

#### Convergence of Gradient Descent

- 학습 속도인 알파가 고정된 경우에도 기울기 하강은 최소값으로 수렴할 수 있다.
  - 계속 하강하다가 최소값 지나치지 않음
  - 우리가 지역 최소값에 가까워질 때 J'(x)의 크기는 **감소**할 것이다.
  - 따라서 미분값 하강은 자동으로 **더 작은** 단계를 밟게 된다.
  - 따라서 시간이 지남에 따라 알파를 줄일 필요 없다.

#### Gradient Descent Algorithm

1. 파라미터 w0, w1, w2, …, wn을 임의로 초기화한다.
   - 임의의 w값 선택
2. 학습률 설정 : 알파 (> 0)
3. 수렴될 때까지 반복한다.

#### Characteristics of Logistic Regression

1. 클래스 조건부 확률에 대한 어떠한 가정도 하지 않고 P(y=1|x)를 직접 계산하는 **판별** 모델
   - 따라서, 그것은 매우 일반적이며 다양한 응용에 적용될 수 있다.
2. 학습된 매개 변수 w0, w1, w2, …, wn을 분석하여 속성과 클래스 레이블 간의 **관계**를 이해할 수 있다.
   - 모든 속성에 대해 서로 다른 가중치가 있기 때문
   - w가 큰 변수는 중요한 변수임
3. 로지스틱 회귀 분석에서는 **관련 없는 속성**을 처리할 수 있다.
   - 무관한 속성에 대한 0에 가까운 가중치 매개 변수를 학습함으로써
4. 로지스틱 회귀 분석에서는 **중복 속성**을 처리할 수 있습니다.
   - 모든 중복 속성에 대해 동일한 가중치를 학습함으로써
   - 따라서 분류 성능이 저하되지 않는다.
5. 로지스틱 회귀 분석에서는 결측값을 처리**할 수 없습니다.**
   - 왜냐하면 P(y|x)는 1/1+e^-z로만 계산되기 때문







## Artificial Neural Network

- **매우 강력한** 분류 모델
  - 매우 복잡하고 비선형적인 의사 결정 경계 학습 가능
  - 많은 애플리케이션에서 널리 수용됨
    - (ex) 시각, 음성 및 언어 처리
  - 종종 다른 분류 모델(심지어 인간)보다 성능이 우수
- **인간의 뇌**를 모방하려는 시도에서 영감을 받았다.

#### Human Brain Model

- 인간의 뇌 구조와 유사하게 ANN은 서로 연결된 다수의 노드로 구성된다.
  - **노드**: 계산의 기본 단위를 수행하는 뉴런
  - **유도 링크**: 뉴런 간의 연결
  - **유도된 링크의 가중치**: 뉴런들 사이의 연결 강도
- ANN의 주요 목표
  - 기본 데이터의 입출력 관계에 **맞도록** 링크의 **가중치**를 조정하는 것

#### Basic Motivation Behind and ANN Model

- 상호 연결된 노드의 복잡한 조합을 사용하여 ANN 모델은 훨씬 풍부한 기능 집합을 **자동으로** 추출할 수 있다
  - 기존의 수작업 기능 추출(예: PCA)과 대비
- 또한, **다양한** 추상화 수준에서 특징을 표현하는 자연스러운 방법을 제공합니다.
  - 복잡한 기능은 단순한 기능의 구성으로 간주된다.
  - (ex) edge(낮은 수준 특징) → 눈, 코, 입술 → 얼굴(높은 수준 특징)

#### The Evolution of ANNs

- perceptron
  - 가장 간단한 ANN모델
- 다층 뉴럴 네트워크
  - 더 복잡한 아키텍쳐
  - 오랜 시간동안 발전을 가로막는 여러가지 문제점에 고통받음
    - (레이어나 노드의 개수를 사람이 정해줘야 했음)
- 딥러닝
  - **심층** 아키텍처로 현대 ANN 모델을 효과적으로 학습할 수 있도록 하는 많은 개발이 이루어짐
  - 그 결과 ANN은 광범위한 인기와 함께 재등장

#### A Major Limitation of a Single Neuron

- 단일 퍼셉트론 또는 로지스틱 회귀 분석에서는 **선형** 결정 경계만 학습한다.
  - 왜냐하면 결정이 w1x1 + ... + wnxn + b > 0 에 달려있음
- 그러나 실제 분류 문제는 종종 **비선형적으로** 분리 가능한 클래스를 포함한다.

#### Combinations of Multiple Neurons

- 단일 뉴런은 OR 함수를 배울 수 있다.
  - 트레이닝 인스턴스를 분리할 수 있는 선형 경계가 있다
  - x1 + x2 - 0.5 >0
- 그러나 XOR함수는 고려
  - 트레이닝 데이터를 나눌만한 선형 경계가 존재하지 않음
  - 따라서 단일 뉴런은 XOR 함수를 배울 수 없다.
- 우리는 XOR 함수를 배우기 위해 **여러개의** 뉴런을 컨택해야 한다.
  - -x1 + x2 - 0.5 > 0
  - x1 - x2 - 0.5 > 0

#### Multi-layer Neural Network

- 뉴런의 기본 개념을 보다 복잡한 노드 아키텍처로 일반화
  - 노드는 **레이어라고** 불리는 그룹으로 배열됩니다.
  - 따라서, **더** 복잡하고 비선형적인 의사 결정 경계를 학습할 수 있다.

- Input layer
  - 입력을 나타냅니다.
- Hidden layers
  - 이전 레이어에서 신호를 수신
  - 활성화 값을 다음 계층으로 전송
  - 원본 데이터로부터 알아서 학습 (사람이 학습한거 아님)
- Output layer
  - 출력 변수의 예측을 생성

#### Power of Multi-layer Neural Network

- 퍼셉트론 또는 로지스틱 회귀 분석(hidden layers **없음**)
  - **하나의** hyperplane(초평면)만 생성할 수 있습니다.
    - 따라서 XOR 문제에 대한 최적의 해결책을 찾을 수 없다
- 다층신경망(**많은** hidden layers)
  - **많은 복잡한** 초평면을 만들 수 있다.
    - 출력 노드는 단순히 숨겨진 노드의 결과를 결합하여 결정 경계를 생성한다.

#### Hidden Layers in ANN

- 분류에 유용한 학습 **잠재 기능**으로 볼 수 있다.
  - Layer 0 : 원본 데이터 (x1, x2, …, xp)
  -  Layer 1: x1, x2, …, xp에서 간단한 속성을 포착
  - Layer 2, …, L – 1: 이들을 결합하여 더 높은 수준의 속성 구성
  - Layer L: 가장 높은 수준의 속성을 결합하여 최종 예측
- 따라서 ANN은 표현력이 **뛰어나다**
  - 그들은 **다른 추상화 수준**에서 속성의 계층 구조를 배울 수 있다.

#### Computation at the Nodes of ANN

- l번째 계층 i번째 노드에서 활성화 값 a(li)
  - w(lij) : 계층(l – 1)의 j번째 노드에서 계층 l의 i번째 노드로의 가중치
  - b(li) : 이 노드의 편향 용어
  - f() : z(li)를 a(li)로 변환하는 활성화 함수

#### Types of Activation Functions

- 다수의 대안적인 활성화 함수가 있다.
  - 이러한 함수는 실제 값과 **비선형** 값을 생성할 수 있다.
  - 각각 다른 특성(예: 파생값)을 가진다.
  - ReLU(0에서 스페셜 핸들링), sigmoid function, Tanh function

#### Learning Model Parameters

- 우리는 J(w, b)를 최소화하기 위해 **gradient descent**을 적용
  - 그러나, 우리는 지역적으로 최적의 해결책에 도달할 수 있다.
- 우리는 각 파라미터 w(lij)와 b(li)를 반복하여 업데이트한다.

#### Computing J(w,b)/w and J(w,b)/b

- 이계산은 특히 hidden layers에서 **사소하지 않다**.
  - w(lij)는 최종 결과 y햇에 직접적으로 영향을 미치지는 않지만 후속 계층에서 활성화 값을 통해 영향을 미치는 복잡한 사슬을 가지고 있기 때문
- Backpropagation
  - 제목의 값을 효과적으로 계산하기 위한 기술
  - 파생된 결과물을 아웃풋 계층에서 hidden 계층으로 역방향 전파를 한다.

#### Learning ANN using Backpropagation

1. D.test에 속해있는 트레이닝 인스턴스(x(k), y(k)) 에서 해라
   1. x(k)값 지나면서 아웃풋 밸류 a값 구하기
   2. 레이어 L부터 1까지 지나면서 델타값 구하기
   3. w와 b의 미분값 구하기 (모든 w와 b에 대해)
2. 모든 가중치와 편향값에 대해서 (w,b) 미분값을 다 더한다
3. w와 b를 업데이트 한다 (gradient descent에 의해)
4. w와 b가 더이상 많이 변하지 않는다면 멈춘다.

#### Characteristics of ANN

1. 다층 신경망은 **보편적인 근사치**이다.
   - **어떤** 목표 함수와도 근사할 수 있다.
   - 따라서 표현력이 뛰어나고 복잡한 의사 결정 경계를 학습할 수 있다.
   - 그러나, 그들은 과적합에 취약하다.
2. ANN은 기능의 **계층**을 표현하는 자연스러운 방법을 제공한다.
3. ANN은 배우기 쉬운 단순한 하위 수준 기능의 **구성**으로 복잡한 상위 수준 기능을 나타낸다.
   - 따라서, 더 많은 숨겨진 레이어와 뉴런을 추가함으로써, ANN에 의해 학습된 복잡한 속성의 수는 전통적인 분류 모델보다 훨씬 더 많다.
4. ANN은 **관련 없는** 속성을 쉽게 처리할 수 있다.
   - 이러한 속성에 대해 가중치를 0으로 사용함
   - 또한 중복 속성은 유사한 가중치를 받고 분류기의 품질을 저하시키지 않는다.
5. 기울기 강하를 통해 얻은 솔루션이 전체적으로 최적임을 **보장하지는 않는다.**
   - J(w,b)는 꽤 복잡한 **볼록하지 않은** 함수이기 때문에
   - 이러한 이유로 ANN은 로컬 최소값에서 고착될 수 있다.
   - 그러나 최적값과 로컬 최소값이 그렇게 큰 차이가 나지 않음(뉴럴 네트워크에서)
6. ANN 교육은 **시간이 많이 걸리는** 과정이다.
   - 특히 숨겨진 노드 수가 많은 경우
   - 따라서 많은 병렬 처리 기술(예: GPU)이 사용된다.





