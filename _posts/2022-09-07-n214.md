---
layout: single
title: "Confusion Matrix, 정밀도와 재현율, ROC곡선"
categories: review
tag: [python, ML]
toc: true

---







# Confusion Matrix

- 혼동행렬, 오차행렬

  - 분류기가 예측한 결과와 실제 결과에 대한 레코드의 개수를 표시한 테이블
  - 예측 오류 및 오류의 유형을 테이블 상에서 확인할 수 있다.

- TP(True Positive) : 실제 Positive인 것을 Positive라고 정확하게 예측한 경우

  - 심장병이 있는 샘플을 심장병이 있다고 예측한 경우

- FP(False Positive) : 실제로는 Negative인데 Positive라고 잘못 예측한 경우

  - 심장병이 없는 샘플을 심장병이 있다고 예측한 경우

- FN(False Negative) : 실제로는 Positive인데 Negative라고 잘못 예측한 경우

  - 심장병이 있는 샘플을 심장병이 없다고 예측한 경우

- TN(True Negative) : 실제 Negative인 것을 Negative라고 정확하게 예측한 경우

  - 심장병이 없는 샘플을 심장병이 없다고 예측한 경우

- ![image](https://user-images.githubusercontent.com/97875918/188900241-ea0710f2-f7d9-45a1-b238-9cdfb1128e43.png)

  













# 정밀도와 재현율

- 타겟이 불균형한 경우 Accuracy는 모델의 성능을 비교하기에 적절하지 않은 평가지표임
  - 정확한 성능 판단을 위해 정확도 외에 다른 평가지표를 봐야한다.
- 정확도 (Accuracy)
  - 전체 범주를 모두 바르게 맞춘 경우를 전체 수로 나눈 값
- 정밀도 (Precision)
  - **Positive로 예측**한 경우 중 올바르게 Positive를 맞춘 비율
- 재현율 (Recall)
  - **실제 Positive**인 것 중 올바르게 Positive를 맞춘 것의 비율 
- F1 점수
  - 정밀도와 재현율의 조화평균















# ROC곡선

- ROC Curve는 여러 임계값에 대해 TPR(True Positive Rate)과 FPR(False Positive Rate)을 그래프로 보여준다.
- Recall(재현율) = Sensitivity(민감도) = TPR = 1 - FNR
- Fall-out(위양성률) = FPR = 1 - TNR
- 재현율을 높이기 위해서는 Positive로 판단하는 임계값을 계속 낮추어 모두 Positive로 판단하게 만들면 된다.
  - 그러나 동시에 위양성률도 같이 높아짐
  - 임계값이 1인 경우 TPR, FPR 모두 0이 된다.
  - 임계값이 0인 경우 TPR, FPR 모두 1이 된다.







## AUC

- ROC 곡선 아래의 면적을 이용한 분류 모델의 성능을 나타내는 지표
- AUC 값이 1에 가까울수록 성능이 좋은 모델이며 0.5에 가까울수록 성능이 안좋은 모델
- AUC score는 모델이 Positive 샘플의 예측값을 Negative 샘플의 예측값보다 크게 줄 확률
- 분류의 결과보다 모델의 예측값 자체가 중요할 경우 많이 사용













