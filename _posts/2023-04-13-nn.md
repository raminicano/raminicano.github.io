---
layout: single
title: "역전파"
categories: CNN
tag: [CNN, Stanford, backpropagation, neural networks]
toc: true

---


본 내용은 스탠포드 대학 cs231n 4강 정리이다.
<br>
<br>

# Backpropagation 

## simple example

![image](https://user-images.githubusercontent.com/97875918/231938109-cb3aa8bf-b43b-410f-abcd-de3ac2bc624a.png)

위와 같은 예시가 있다. 어떻게 gradient를 이용해 가중치를 업데이트를 시킬지를 배우기 위해 backpropagation 개념이 등장한다. <br>

![image](https://user-images.githubusercontent.com/97875918/231939293-bbe70ad5-c259-4401-a042-b36a9167152e.png)

우리가 알아야할 것은 x,y,z 가 f에 미치는 영향이다. 이걸 미분 식으로 표현하면 df/dx, df/dy, df/dz 로 표현한다. 이는 미분으로 구할 수 있다.

![image](https://user-images.githubusercontent.com/97875918/231939040-188c8c19-4f47-4f76-90ac-f22b492f3859.png) 

F가 f에 영향을 미치는 것은 당연히 1이다.

![image](https://user-images.githubusercontent.com/97875918/231939237-5a01025a-d3fc-4c81-be06-e936a5a1fd40.png)

앞의 식에서 구해놨듯이 저 미분값은 q값이 되기 때문에 3이 된다.

![image](https://user-images.githubusercontent.com/97875918/231939666-eda838d0-86bd-4c6c-b171-323f9cc0fc84.png)

마찬가지로 q가 f에 영향을 미치는 것은 z가 될 것이고 -4가 된다. 
<br>
<br>


### chain rule

![image](https://user-images.githubusercontent.com/97875918/231939853-bfd7547a-0922-4c4e-9650-f61c261b670d.png)

여기서 y가 f에 미치는 영향과 x가 f에 미치는 영향 값을 구해야되는데 바로 구할 수가 없다. (계산 안됨) 그래서 <b><u>chain rule</u></b> 이 적용된다. <br>
만약 x가 f에 미치는 영향을 알고 싶으면 x-> q-> f 순으로 계산되기 때문에 
(x가 q에 미치는 영향) * (q가 f에 미치는 영향) 을 구하면 된다. 여기서 q가 f에 미치는 영향은 이미 구했고 x가 q에 미치는 영향은 1이기 때문에 값을 구할 수 있다.

<br> <br>

y가 q에 미치는 영향도 1임을 이미 구했기 때문에 1 * (q가 f에 미치는 영향 = z = -4) 이므로 -4가 된다. (x도 마찬가지로 -4가 된다.)


<br>
<br>
<br>

### local gradient, global gredient

<b><u>local gradient</u></b> : 미리 구할 수 있는 미분 값 (ex. qz, x+y) <br>
<b><u>global gradient</u></b> : 앞에서 넘어온 미분 값

<img width="751" alt="image" src="https://user-images.githubusercontent.com/97875918/231978873-f751cf18-0830-4e8f-bae2-d7a0b2fcbae4.png">


이와 같은 과정을 거친 gradient를 다시 뒤로 보내고 (global gradient값이 됨) 뒤에 있는 local gradient와 다시 계산을 하는 과정을 반복한다. (두개의 값을 곱해서 전달함)

<br>
<br>
<br>
<br>

## another example


<img width="713" alt="image" src="https://user-images.githubusercontent.com/97875918/231979031-c0ec19cf-6b36-4c7d-815d-87b2e72397d8.png">



조금 더 심화된 내용으로 시그모이드 함수의 역전파 과정을 계산해보자. 다음과 같이 계산하기 편하도록 4가지의 local gradient의 식을 구했다.


<img width="391" alt="image" src="https://user-images.githubusercontent.com/97875918/231979163-3081e0b7-2b63-4232-b099-3e008460c668.png">


맨 처음 gradient는 당연히 1이다. <br>
- 1.37 기준 
- gloabl gradient는 1이고  
- local gradient : 1/x 를 미분한 것 즉 -1/x^2가 된다. 
- 이 두개의 값을 곱하면 gradient가 되기 때문에 이 값을 구하면 -0.53이다.

<img width="371" alt="image" src="https://user-images.githubusercontent.com/97875918/231979317-1444b0f0-5879-4c34-aa8e-a4163aea67f7.png">


- 0.37 기준
- gloabl gradient : -0.53
- local gradient : x+1를 미분한 것 즉 1이다.
- 두개를 곱하면 -0.53

<img width="570" alt="image" src="https://user-images.githubusercontent.com/97875918/231979441-4a8c56a1-0dd3-45db-bb7f-0f9ae4e31bbc.png">

- -1 기준
- gloabl gradient : -0.53
- local gradient : e<sup>x</sup> 를 미분하면 e<sup>x</sup> 이므로 여기서 x가 -1이기 때문에 e<sup>-1</sup>이다.
- 두개를 곱하면 -0.2

<img width="443" alt="image" src="https://user-images.githubusercontent.com/97875918/231980718-4efaadb2-f29f-4655-882d-3f512b566003.png">


- 1 기준
- gloabl gradient : -0.2
- local gradient : ax를 미분하면 a가 나온다. 즉 -1이다.
- 두개를 곱하면 0.2


<img width="596" alt="image" src="https://user-images.githubusercontent.com/97875918/231981023-9bbfdd4b-1ab2-4644-a991-2f26291deaac.png">

<img width="597" alt="image" src="https://user-images.githubusercontent.com/97875918/231981684-45b9f565-7844-4f95-9d92-07881ffa74b8.png">


### sigmoid gate

<img width="560" alt="image" src="https://user-images.githubusercontent.com/97875918/231984193-5c63bdb1-3d97-45a5-ad87-93dec5e8db9e.png">


- 앞에서는 하나하나 구했지만 위와 같은 미분의 과정을 거치면 하나의 식이 나온다.
- 이를 일반화한 값이 <u><b>(1-sigmoid) * sigmoid</b></u>로 표현된다.

<img width="649" alt="image" src="https://user-images.githubusercontent.com/97875918/231984722-5c777dac-2f76-490d-9955-85ba5279cd46.png">

- 즉 sigmoid 게이트에 들어가기 전 gradient는 시그모이드 값인 0.73을 기준으로 (1-0.73) * 0.73 으로 구할 수 있다. 


<br>
<br>
<br>
<br>


## Patterns in backward flow

<img width="697" alt="image" src="https://user-images.githubusercontent.com/97875918/231992014-5cc75766-1db0-4639-8eb3-0dd2127f7427.png">

- add gate : add의 경우는 local gradient가 1이다. 그래서 global gradient * local gradient 를 하게되면 global gradient가 나온다. 즉, add gate는 gradient를 그대로 전해주는 역할을 하게되어 <u><b>distributor</b></u>라고 불린다.
- max gate : max의 기능처럼 큰 값에게 gradient를 전하고 작은 값은 0으로 만들어서 보낸다.
- mul gate : 예를들어 qz라는 함수가 있다고 보면 미분 시 q는 z값을 가지게 되고 z는 q의 값을 가지게 된다. 이렇게 각각의 값이 스위칭 되기 때문에 mul gate는 <u><b>switcher</b></u>라고 불린다.


<br>
<br>
<br>
<br>

## Gradients add at branches

앞에서 온 gradient가 1개가 아니라 여러개면? -> 복수의 gradient를 더해주면 된다. <br>

그러나 하나의 변수가 아니라 다변수일 확률이 높다. -> 자코비안 행렬(다변수 함수일때의 미분값) 방식을 사용한다.

<img width="645" alt="image" src="https://user-images.githubusercontent.com/97875918/234486285-0abfbfcd-2b8a-4f71-aa73-de01deed4eef.png">


위와 같이 input이 d차원의 벡터가 들어가게 된다. 따라서 output도 d차원이다. <br>
jacobian matrix는 4096 * 4096 가 된다.<br>

여기에 minibatch를 추가하게 되면 (ex. 100개씩 추가) 크기는 409600 * 409600이 된다.


### 구체적인 예시

<img width="715" alt="image" src="https://user-images.githubusercontent.com/97875918/234488511-0cfd6910-896e-4893-98e8-7d277c661036.png">

<img width="588" alt="image" src="https://user-images.githubusercontent.com/97875918/234489682-7f3a806b-7f49-4865-918c-41c8ef9bc5ea.png">


위와 같은 예시가 있다고 하자. q=wx이고, w와 x의 각 값이 저런 값을 가진다고 보고 계산을 해보면
- *부분의 값
    - 0.22 : -0.1 * 0.2 + 0.5 * 0.4
    - 0.26 : -0.3 * 0.2 + 0.8 * 0.4
- L2부분의 값
    - 0.116 : *부분의 값을 각각 제곱해서 더한 값 


#### 미분하기

<img width="390" alt="image" src="https://user-images.githubusercontent.com/97875918/234501326-fc27aebb-0b5c-430e-8543-83050993479b.png">

- L2
    - 자기자신 미분하면 1임
- 곱하기 * 
    - q<sub>i</sub>가 df에 미치는 영향은 2q<sub>i</sub>
    - 2q값을 취하기
    - local gradient(2q) * global(1)
    - 그래서 0.22 * 2, 0.26 * 2


<br>
<br>

![image](https://user-images.githubusercontent.com/97875918/236612022-4452e4fc-a916-4074-9477-9e96723920c8.png)

- 체인룰을 이용해서 위와 같은 식이 유도된다.




# 요약
- 신경망은 매우 커진다. : 모든 파라미터에 대한 공식을 손으로 쓰는 것은 비현실적이다.
- 역전파 : 계산 그래프를 따라 체인룰을 반복적으로 적용해 모든 입력, 파라미터, 중간값의 경사를 계산
- 구현은 그래프 구조를 유지하며 노드들은 forward(), backward() API를 통해 구현한다.
- forward : 연산의 결과를 계산하고 경사 계산에 필요한 중간값을 메모리에 저장한다.
- backward : 체인 룰을 적용해 입력에 대한 손실함수의 경사를 계산한다.

