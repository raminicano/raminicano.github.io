---
layout: single
title: "Isolation Forest 설명하기"
categories: AI
tag: [인공지능, 이상탐지, 머신러닝, 딥러닝]
toc: true

---


# 개념
Isolation Forest는 Unsupervised Anomaly Detection 중 하나로 현재 갖고 있는 데이터 중 이상치를 탐지할 때 주로 사용된다. 여러개의 의사결정 나무를 종합한 앙상블 기반의 이상탐지 기법으로 의사결정나무를 지속적으로 분기시키면서 모든 데이터 관측치의 고립 정도 여부에 따라 이상치를 판별하는 방법이다.
- 핵심 아이디어 : 정상적인 관측치라면 고립시키기 어려울 것이고 이상한 관측치라면 고립시키기 쉬울 것이다.
- tree 기반으로 구현
- 랜덤으로 데이터를 split하여 모든 관측치를 고립시키며 구현
- 변수가 많은 데이터에서도 효율적으로 작동할 수 있다.

<u><b>원리</b></u>
<img width="720" alt="image" src="https://user-images.githubusercontent.com/97875918/230599292-a0982188-15db-4174-8500-08b8ef87c7d6.png">

- 비정상 데이터 : 의사결정 나무의 루트에서 가까운 깊이에서 고립됨
- 정상 데이터 : 루트에서 먼 깊이에서 고립됨
- 특정한 샘플이 고립되는 leaf노드까지의 거리를 outlier score로 정의
- 루트 노드까지의 평균 거리가 짧을 수록 outlier score가 높아진다.

<br>
<br>
<br>
<br>


# Algorithm

<i><b>iTree</b></i>

1. Sub-sampling : 비복원 추출로 데이터 중 일부를 샘플링
2. 변수 선택 : 데이터 X의 변수 중 q를 랜덤 선택
3. split point 설정 : 변수 q의 범위 (max~min) 중 uniform하게 split point를 선택
    - 이상치 여부 판단 : 각 나무 별 데이터가 고립되기까지의 평균길이 (루트 노드로부터의 depth)를 계산한다.
4. 1~3번 과정을 모든 관측치가 split되거나, 임의의 split 횟수까지 반복(=재귀나무)하며, 경로 길이를 모두 저장
    - 샘플링된 데이터와 선택된 변수를 활용해 다양한 나무를 생성해 분기한 뒤에는 각 나무별로 데이터의 leaf node를 알 수 있다.
    - leaf node에 포함된 데이터가 1이라면 고립이 된 것
    - 1보다 크다면 최대 깊이로 한정되어 더 이상 분기하지 못한 것

<i><b>iForest</b></i>

5. 1~4번 과정(iTree)을 여러번 반복
    - iTree가 아닌 iForest여야 하는 이유
    <img width="373" alt="image" src="https://user-images.githubusercontent.com/97875918/230859283-edfd3cdf-427f-43bd-916c-a483851ab2eb.png">
        - tree의 개수가 늘어날 수록 정상치든 이상치든 각각의 경로 길이의 평균이 일정한 값으로 수렴한다.
        - 나무를 많이 쓸수록 데이터 포인트의 평균 경로 길이가 안정적이게 된다.


<br>
<br>
<br>
<br>

# Scoring
<img width="213" alt="image" src="https://user-images.githubusercontent.com/97875918/230606412-fc37d5fb-fc17-41ba-957e-a1f59b9821df.png">

- h(x) : 해당 관측치의 경로길이
- E(h(x)) : 모든 iTree에서 해당 관측치에 대한 평균 경로길이
- c(n) : h(x)를 normalize하기 위한 값으로 iTree의 평균 경로 길이

이와 관련해 Score값의 분포를 알아보자
1. 관측치 x가 전체 경로길의 평균과 유사 (=정상 데이터) : E(h(x)) -> c(n), s -> 0.5
2. 관측치 x가 이상치 : E(h(x)) -> 0, s -> 1
3. 관측치 x의 최대 경로길이 : E(h(x)) -> n-1, s-> 0

즉 score는 0~1 사이에 분포되며 1에 가까울수록 이상치일 가능성이 크고 0.5이하이면 정상데이터로 판단할 수 있다.


<br>
<br>
<br>
<br>

# 장점

- 계산량이 적고 속도가 빠르다.
    - 다른 이상치 탐지 방식은 데이터의 밀도나 거리를 계산하는 방식으로 접근힌다. -> 계산비용이 높아짐
- 이상치 여부 1또는 0 만이 아니라 이상치 점수를 산출해주기 때문에 사용자가 원하는 cut-off 값을 적용할 수 있다.
- 학습 데이터에 꼭 이상치가 포함되어 있지 않아도 잘 적용된다.

<br>
<br>
<br>
<br>


# In python

파이썬에서는 sklearn.ensemble 모듈의 IsolationForest함수가 Isolation Forest 알고리즘을 지원한다.

<b><u>Parameters</u></b>

- n_estimators : 나무의 개수로 디폴트는 100
- max_samples : split 숫자가 정수일 경우 지정한 정수 개수만큼 각 나무별로 데이터를 샘플린한다. 0~1사이의 실수일 경우 전체 데이터 개수에 실수를 곱한 만큼의 데이터를 사용한다. 따로 지정하지 않은 경우 전체 데이터 개수와 256 중 작은 수를 사용한다.
- contamination : 전체 데이터에서 이상치의 비율이다. 이 비율에 따라 이상치로 판단하기 위한 score의 threshold를 정의한다.
- max_features : 각 나무를 훈련할 때 사용할 feature의 개수로 디폴트는 1로 선언되어 있다.
- contamination : 전체 데이터에서 (예상되는/지정된) 이상치 비율
    - 사실상 이 값이 이상치 점수의 threshold를 정하는 역할을 한다.
    - auto : 0.5로 자른다. 이렇게 한 다음 최종 데이터를 살펴보고 threshold를 정하는 방식을 추천한다.


<br>
<br>
<br>
<br>