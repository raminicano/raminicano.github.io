---
layout: single
title: "머신러닝 프로젝트 과정"
categories: Hands-On-ML
tag: [ML]
toc: true
---

# 1. 큰 그림을 본다.

1. 목표를 비즈니스 용어로 정의한다.
2. 이 솔루션은 어떻게 사용될 것 인가?
3. (만약 있다면) 현재 솔루션이나 차선책은 무엇인가?
4. 어떤 문제라고 정의할 수 있나 (지도/비지도, 온라인/오프라인)?
   - 지도
     - 알고리즘에 주입하는 훈련데이터에 **레이블(lable)**이라는 원하는 답이 포함된다.
     - 분류, 회귀
   - 비지도
     - 훈련데이터에 레이블이 없다.
     - 군집, 이상치탐지와 특이치탐지, 시각화와 차원축소, 연관규칙학습
   - 준지도학습
     - 일부만 레이블이 있는 데이터를 다룬다.
     - 지도학습과 비지도학습의 조합
   - 강화학습
     - 에이전트(학습하는 시스템)이 환경을 관찰해 행동을 실행하고 그 결과로 보상 or 벌점을 받는다.
     - 시간이 지나면서 가장 큰 보상을 얻기 위해 정책(최상의 전략)을 스스로 학습한다.
   - 온라인
     - 데이터를 순차적으로 한 개씩 또는 미니배치(작은 데이터 묶음 단위)로 주입하여 시스템을 훈련시킨다.
     - 매 학습단계가 빠르고 비용이 적게 들어 데이터가 도착하는대로 즉시 학습 가능 
   - 오프라인
     - 배치학습
       - 시스템이 점진적으로 학습할 수 없음
       - 시간과 자원을 많이 소모해 보통 오프라인에서 수행된다.
     - 학습한 것을 단지 적용만 함
5. 성능을 어떻게 측정해야 하나?
   - 평균제곱근오차(RMSE)
     - 값이 커질수록 예측에 오류가 많음
   - 평균절대오차(MAE)
     - RMSE를 쓰기엔 이상치가 많은 경우
6. 성능지표가 비즈니스 목표에 연결되어 있나?
7. 비즈니스 목표에 도달하기 위해 필요한 최소한의 성능은 얼마인가?
8. 비슷한 문제가 있나? 이전의 방식이나 도구를 재사용할 수 있나?
9. 해당 분야의 전문가가 있나?
10. 수동으로 문제를 해결하는 방법은 무엇인가?
11. 나 또는 타인이 세운 가정을 나열한다.
12. 가능하면 가정을 검증한다.

# 2. 데이터를 구한다.

1. 필요한 데이터와 양을 나열한다.

2. 데이터를 얻을 수 있는 곳을 찾아 기록한다.

3. 얼마나 많은 공간이 필요한지 확인한다.

4. 법률상의 의무가 있는지 확인하고 필요하다면 인가를 받는다.

5. 접근 권한을 획득한다.

6. 작업 환경을 만든다.

7. 데이터를 수집한다.

   - head(), info(), value_counts(), describe(), hist() 등을 사용해 데이터 구조를 훑어본다.
   - 열 이름, 데이터형, null값 

8. 데이터를 조작하기 편리한 형태로 변환한다.

9. 민감한 정보가 삭제되었거나 보호되었는지 검증한다.

10. 데이터의 크기와 타입(시계열, 표본, 지리정보 등)을 확인한다.

11. 테스트 세트를 샘플링하여 따로 떼어놓고 절대 들여다보지 않는다.

    - 데이터 스누핑(data snooping) 편향

      - 테스트 세트로 일반화 오차를 추정하면 매우 낙관적인 추정이 되어 시스템을 론칭했을 때 기대한 성능이 나오지 않음

    - 무작위 샘플링

      - ```python
        from sklearn.model_selection import train_test_split
        train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
        ```

    - 계층적 샘플링

      - pd.cut() 으로 계층을 나눠줌 (데이터 편향 방지)

      - ```py
        from sklearn.model_selection import StratifiedShuffleSplit
        
        split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
        for train_index, test_index in split.split(housing, housing["income_cat"]):
            strat_train_set = housing.loc[train_index]
            strat_test_set = housing.loc[test_index]
        ```



# 3. 데이터로부터 통찰을 얻기 위해 탐색하고 시각화 한다.

1. 데이터 탐색을 위해 복사본을 생성한다.
2. 데이터 탐색 결과를 저장하기 위해 주피터 노트북을 만든다.
3. 각 특성의 특징을 조사한다.
   1. 이름
   2. 타입(범주형, 정수/부동소수, 최댓값/최솟값 유무, 텍스트, 구조적인 문자열 등)
   3. 누락된 값의 비율(%)
   4. 잡음 정도와 잡음의 종류(확률적, 이상치, 반올림 에러 등)
   5. 이 작업에 유용한 정도
   6. 분포 형태(가우시안, 균등, 로그 등)
4. 지도 학습 작업이라면 타깃 속성을 구분한다.
5. 데이터를 시각화한다.
   - ex. 지리적 데이터 시각화
6. 특성 간의 상관관계를 조사한다.
   - 표준상관계수 (피어슨)
     - 데이터셋이 너무 크지 않은 경우 사용가능 
     - `corr()` 메서드 사용
   - 숫자형 특성 사이에 산점도 그리기
     - `scatter_matrix()` 함수 사용
7. 수동으로 문제를 해결할 수 있는 방법을 찾아본다.
8. 적용 가능한 변환을 찾는다.
   - 정제해야할 이상한 데이터 확인
   - 특성사이 흥미로운 상관관계 발견
   - 꼬리 두꺼운 분포라서 변형해야 됨
   - ex . 특정 구역의 방 개수보다 가구당 방 개수가 더 유용함
9. 추가로 유용한 데이터를 찾는다(있으면 데이터 수집과정으로 돌아감)
10. 조사한 것을 기록한다.

# 4. 머신러닝 알고리즘을 위해 데이터를 준비한다.

- 데이터의 복사본으로 작업한다.(원본 데이터는 유지)
- 적용한 모든 데이터 변환은 함수로 만든다
  1. 다음에 새로운 데이터 얻을 때 데이터 준비를 쉽게 함
  2. 다음 프로젝트에 이 변환을 쉽게 적용
  3. 테스트 세트를 정제하고 변환하기 위해
  4. 솔루션이 서비스에 투입된 후 새로운 데이터 샘플을 정제하고 변환하기 위해
  5. 하이퍼파라미터로 준비 단계를 쉽게 선택하기 위해



1. 데이터 정제

   - 이상치를 수정하거나 삭제(선택)

   - 누락된 값을 채우거나(ex. 0, 평균, 중간값) 행 또는 열을 제거

   - `drop(), dropna(), fillna()`

   - ```python
     from sklearn.impute import SimpleImputer
     imputer = SimpleImputer(strategy="median")
     housing_num = housing.drop("ocean_proximity", axis=1) # 중간값은 수치형 특성에만 가능하기에 제거
     imputer.fit(housing_num) # 각 특성의 중간값을 계산해 그 결과를 객체의 statistic_ 속성에 저장
     X = imputer.transform(housing_num) # 변형된 특성들이 들어있는 평범한 넘파이 배열임
     housing_tr = pd.DataFrame(X, columns=housing_num.columns,index=housing_num.index)
     ```

2. 특성 선택(선택사항)

   - 작업에 유용하지 않은 정보를 가진 특성을 제거

3. 적절한 특성 공학

   - 연속 특성 이산화하기

   - 특성 분해하기(ex. 범주형, 날짜/시간)

     - 임의의 텍스트를 범주형 특성으로 변환

       - 사이킷런의 OrdinalEncoder

         - ```python
           housing_cat = housing[["ocean_proximity"]]
           from sklearn.preprocessing import OrdinalEncoder
           ordinal_encoder = OrdinalEncoder()
           housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
           # 카테고리들의 배열을 담은 리스트가 반환됨
           ordinal_encoder.categories_
           #[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],dtype=object)]
           ```

         - 문제점 : 실제로 카테고리 0과 1보다 0과 4가 더 연관되어 있는 것에 비해 알고리즘은 전자를 더 비슷하다고 생각한다.

       - 사이킷런의 One-hot encoding

         - 한 특성만 1이고(핫) 나머지는 0임

         - ```python
           from sklearn.preprocessing import OneHotEncoder
           cat_encoder = OneHotEncoder()
           housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
           housing_cat_1hot #희소행렬을 반환함(넘파이 배열에 비해 메모리 절약)
           housing_cat_1hot.toarray() # 밀집 배열로 변환
           ```

   - 가능한 특성 변환 추가하기 (ex. log(x), squrt(x), x^2)

   - 특성을 조합해 가능성 있는 새로운 특성 만들기

4. 특성 스케일 조정

   - 표준화
     - `StandardScaler()`
     - 신경망의 경우 0-1사이의 범위를 기대함
   - 정규화
     - `MinMaxScaler()`
     - 이상치에 민감





# 5. 모델을 선택하고 훈련시킨다.

- 데이터가 매우 크면 여러 모델을 일정 시간 안에 훈련시킬 수 있도록 데이터를 샘플링하여 작은 훈련 세트를 만드는 것이 좋음
- 가능한 최대로 이 단계를 자동화한다.



1. 여러 종류의 모델을 기본 매개변수를 사용해 신속+많이 훈련시킨다.

   - ```python
     # 선형회귀모델 만듦
     from sklearn.linear_model import LinearRegression
     lin_reg = LinearRegression()
     lin_reg.fit(housing_prepared, housing_labels)
     
     # 전체 훈련세트에 대한 회귀 모델의 RMSE 측정
     from sklearn.metrics import mean_squared_error
     housing_predictions = lin_reg.predict(housing_prepared)
     lin_mse = mean_squared_error(housing_labels, housing_predictions)
     lin_rmse = np.sqrt(lin_mse)
     lin_rmse
     ```

2. 성능을 측정하고 비교한다

   - 각 모델에서 K-겹 교차 검증을 사용해 K개 폴드의 성능에 대한 평균과 표준편차를 계산

     - 훈련세트에서 더 작은 훈련세트와 검증세트로 나누고 더 작은 훈련세트에서 모델을 훈련시키고 검증세트로 모델을 평가함

     - ```python
       # 10개의 서브셋 무작위 분할 후 매번 다른 폴드를 선택해 평가하고 나머지 9개 폴드는 훈련에 사용
       from sklearn.model_selection import cross_val_score
       tree_reg = DecisionTreeRegressor(random_state=42)
       scores = cross_val_score(tree_reg, housing_prepared, housing_labels,scoring="neg_mean_squared_error", cv=10)
       tree_rmse_scores = np.sqrt(-scores)
       ```

     - 사이킷런의 교차검증 기능은 scoring 매개변수에 낮을수록 좋은 비용함수가 아니라 클수록 좋은 효용함수를 기대한다. 따라서 평균 제곱오차(MSE)의 반댓값(즉 음수임)을 사용한다.

       - 그래서 비용함수로 바꿈

3. 각 알고리즘에서 가장 두드러진 변수를 분석한다.

4. 모델이 만드는 에러의 종류를 분석한다

   - 에러를 피하기 위해 사람이 사용하는 데이터가 무엇인지 파악

5. 간단한 특성 선택과 특성 공학 단계를 수행한다.

6. 이전 다섯 단계를 1-2번 빠르게 반복한다.

7. 다른 종류의 에러를 만드는 모델을 중심으로 가장 가능성 높은 모델을 3-5개 정도 추린다.







# 6. 모델을 상세하게 조정한다.

- 이 단계에서는 가능한 많은 데이터를 사용하는 것이 좋다
- 자동화 요망



1. 교차 검증을 사용해 하이퍼파라미터를 정밀튜닝한다.

   - 그리드 탐색

     - 사이킷런의 GridSearchCV 사용

     - ```python
       from sklearn.model_selection import GridSearchCV
       
       param_grid = [
           # 12(=3×4)개의 하이퍼파라미터 조합을 시도합니다.
           {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
           # bootstrap은 False로 하고 6(=2×3)개의 조합을 시도합니다.
           {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
         ]
       
       forest_reg = RandomForestRegressor(random_state=42)
       # 다섯 개의 폴드로 훈련하면 총 (12+6)*5=90번의 훈련이 일어납니다.
       grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                                  scoring='neg_mean_squared_error',
                                  return_train_score=True)
       grid_search.fit(housing_prepared, housing_labels)
       # 최상의 파라미터 조합
       grid_search.best_params_
       # {'max_features': 8, 'n_estimators': 30} 근데 이건 탐색 범위의 최댓값이기에 점수가 향상될 가능성이 있어 더 큰 값으로 다시 검색해보자
       ```

   - 랜덤 탐색

     - 그리드 탐색은 비교적 적은 수의 조합을 탐구할 때 괜찮지만 하이퍼 파라미터의 탐색공간이 커지면 랜덤탐색사용
     - `RandomizedSearchCV`

2. 앙상블 방법을 시도해본다.

   - 최고의 모델들을 연결하면 종종 개별 모델을 실행하는 것보다 성능 높음

3. 최종 모델에 확신이 선 후 일반화 오차를 추정하기 위해 테스트 세트에서 성능을 측정한다.





# 7. 솔루션을 제시한다.

1. 지금까지의 작업을 문서화
2. 발표자료 만들기
   - 큰 그림을 부각시키기
3. 이 솔루션이 어떻게 비즈니스의 목표를 달성하는지 설명
4. 작업 과정에서 알게 된 흥미로운 점들을 잊지말고 설명
   - 성공한 것과 그렇지 못한 것을 설명
   - 내가 세운 가정과 시스템의 제약을 나열
5. 그래프나 기억하기 쉬운 문장으로 핵심 내용을 전달





# 8. 시스템을 론칭하고 모니터링하고 유지보수한다.

1. 서비스에 투입하기 위한 솔루션을 준비
   - ex. 실제 입력데이터 연결, 단위 테스트 작성 등
2. 시스템의 서비스 성능을 일정한 간격으로 확인하고 성능이 감소됐을 때 알림을 받기 위해 모니터링 코드를 작성한다.
3. 정기적으로 새로운 데이터에서 모델을 다시 훈련시킨다.





