---
layout: single
title: "이상 탐지 정의"
categories: AI
tag: [인공지능, 이상탐지, 머신러닝, 딥러닝]
toc: true


---


Anomaly Detection은 
- 학습 데이터 셋에 비정상적인 sample이 포함되는지 
- 각 sample의 성격이 정상 sample과 어떻게 다른지
- 정상 sample의 class가 단일 class인지 Multi-class인지
등에 따라 다른 용어를 사용한다. 이와 <a href="https://github.com/hoya012/awesome-anomaly-detection"> 관련한 논문 </a> 이며 참고하는 것도 좋을 듯하다.

<br>
<br>
<br>

# 1. 학습시 비정상 sample 의 사용여부 및 label 유무에 따른 분류
## Supervised Anomaly Detection
- 주어진 학습 데이터 셋에 정상과 비정상 sample의 Data와 Label이 모두 존재하는 경우 

<u>장점</u>
- Supervised Learning 방식은 다른 방법 대비 정확도가 높다.
    - 그래서 높은 정확도를 요구할 때 주로 사용됨
- 비정상 sample을 다양하게 보유할 수록 더 높은 성능을 달성할 수 있음
- 양/불 판정 정확도가 높다.

<u>단점</u>
- Anomaly Detection이 적용되는 일반적인 산업 현장에서는 정상 sample보다 비정상 sample의 발생 빈도가 현저히 적기 때문에 <b>Class-Imbalance(불균형)</b> 문제를 자주 겪게 된다.
- 비정상 sample을 취득하는데 시간, 비용이 많이 든다.

<u>방법론</u>
- Data Augmentation(증강), Loss function 재설계, Batch Sampling 등의 다양한 연구

<br>
<br>

## Semi-supervised (One-Class) Anomaly Detection

Class-Imbalance가 매우 심한 경우 정상 sample만 이용해 모델을 학습 시킨다. <br>
<u>핵심 아이디어</u>
- 정상 sample들을 둘러싸는 discriminative boundary를 설정
- 이 boundary를 최대한 좁혀 boundary 밖에 있는 sample들을 모두 비정상으로 간주한다.

<u>대표적인 방법론</u>
- One-Class SVM
- Deep SVDD
- Energy-based 방법론 “Deep structured energy based models for anomaly detection, 2016 ICML”
- Deep Autoencoding Gaussian Mixture Model 방법론 “Deep autoencoding gaussian mixture model for unsupervised anomaly detection, 2018 ICLR” 
- Generative Adversarial Network 기반 방법론 “Anomaly detection with generative adversarial networks, 2018 arXiv”
- Self-Supervised Learning 기반 "Deep Anomaly Detection Using Geometric Transformations, 2018 NeurIPS"

<u>장점과 단점</u>
- 장점 : 비교적 활발하게 연구가 진행되고 있으며 정상 sample만 있어도 학습이 가능하다.
- 단점 : supervised anomaly detection 방법론과 비교했을 때 상대적으로 양/불 판정 정확도가 떨어진다.

<br>
<br>

## Unsupervised Anomaly Detection
<u>핵심 아이디어</u> 
- 위의 방식들은 정상 sample이 필요하다. 
- 수많은 데이터 중 어떤 것이 정상인지 알려면 그에 대한 label을 확보하는 과정이 필요하다. 
- 대부분의 데이터가 정상 sample이라는 가정을 해 label 취득 없이 학습을 시키는 Unsupervised Anomaly Detection 방법론
    - 주어진 데이터에 대해 PCA 를 이용해 차원을 축소하고 복원하는 과정을 통해 비정상 sample을 검출한다.
    - Neural Network기반으로는 대표적으로 Autoencoder기반의 방법론이 주로 사용됨
        - 장점 : 데이터에 대한 labeling을 하지 않아도 데이터의 주성분이 되는 정상 영역의 특징들을 배울 수 있다.
        - 단점1 : Autoencoder의 압축 정도(=code size = latent variable) 같은 hyper-parameter에 따라 전반적인 복원 성능이 좌우되기 때문에 양/불 판정 정확도가 Supervised Anomaly Detection에 비해 다소 불안정하다.
        - 단점2 : autoencoder에 넣어주는 input과 output의 차이를 어떻게 정의할 것인지 (=어떤 방식으로 difference map을 계산할지) 어느 loss function을 사용해 autoencoder를 학습시킬지 등 여러가지 요인에 따라 성능이 크게 달라질 수 있다.
            -> 즉 양/불 판정 정확도가 높지 않고 hyper parameter에 매우 민감하다.

<br>
<br>
<br>
<br>


# 2. 비정상 sample 정의에 따른 분류

비정상 sample을 정의하는 방식에 따라 크게 Novelty Detection과 Outlier Detection으로 구분한다. 
<p><u>예시</u><br>
강아지를 normal class로 정의한 경우
- 현재 보유 중인 데이터 셋에 이전에 없던 형태의 새로운 강아지가 등장하는 경우
    - 이런 sample을 <b>Novel Sample, Unseen Sample </b>등으로 부를 수 있다.
    - 이런 sample을 찾아내는 방법론을 <b>Novelty Detection</b>이라 부를 수 있다.
    - 즉, 지금까지 등장하지 않았지만 충분히 등장할 수 있는 sample을 찾아내는 연구
    - 데이터가 오염이 되지 않은 상황을 가정하는 연구와 관련된 용어
- 새로운 sample이 등장했지만 호랑이, 말, 운동화, 비행기 등 강아지와 전혀 관련 없는 sample이 등장한다고 가정
    - 이런 sample들을 <b>Outlier sample, 혹은 Abnormal sample</b>이라 부른다.
    - 이러한 sample을 찾아내는 문제를 <b>Outlier Detection</b>이라 부를 수 있다.
    - 즉, 등장할 가능성이 거의 없는
    - 데이터에 오염이 발생했을 가능성이 있는 sample을 찾아 내는 연구와 관련된 용어

</p>


<br>
<br>
<br>
<br>


# 3. 정상 sample의 class 개수에 따른 분류
2번의 경우 데이터 셋이 정상 sample이 단일 class로 구성되어 있고 단순 양/불 판정을 하는 경우에만 가정했지만 실제 환경에서는 정상 sample이 여러 개의 class로 구성될 수 있다.
- 그러나 정상 sample이 Multi- Class인 상황에서도 위의 Novelty Detection, Outlier Detection 기준을 똑같이 적용할 수 있다.
- 이런 경우 정상 sample이라는 표현 대신 In-distribution sample이라는 표현을 사용한다.

<u>Out-of-distribution Detection</u>
- In-distribution 데이터 셋으로 network를 학습시킨 뒤 test 단계에서 비정상 sample을 찾는 문제


<br>
<br>
<br>
<br>

# 정리
![image](https://user-images.githubusercontent.com/97875918/230376397-50e29546-104f-474f-923c-ced4486907fe.png)


<br>
<br>
<br>
<br>

