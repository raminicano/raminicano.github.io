---
layout: single
title: "CH1 and 2 : Introduction and Data"
categories: coding
---



# ch1

#### **Introduction**

- 지금의 데이터는 분석하기에 너무 크다.
- \- 3Vs of big data : 용량, 속도, 다양성



#### **Data Mining**

- 커다란 데이터 sets에서 유용한 정보를 추출하는 과정
- 전통적 통계학 + 최신 빅데이터 기술



#### **Applications : Buiness and Industry**

- Pos 데이터 분석, 자동 매매, 고객 프로파일링, 타깃 마케팅, 스토어 레이아웃, 부정행위 검출
- **인터넷** : 제품추천, 스팸 필터링, 소셜 커넥션 제안
- **모바일 센서와 디바이스** : 가정용 시스템 설계, 스마트시티 도시계획



#### **Applications : Science and Engineering**

- 인공위성에 의해 수집된 지구 데이터 : 관측치 간의 관계를 분석
- 대량의 게놈 데이터 : 각 유전자의 기능을 분석, 단백질 구조를 예측
- 전자 건강 기록 데이터 : 보다 개인화된 환자 관리를 제공하기 위해 사용



#### **What is Data Mining?**

- 데이터 마이닝 : 대용량 데이터에서 유용한 정보를 **자동으로** 검색하는 프로세스, 알 수 없는 새롭고 유용한 패턴 찾기
- 모든 정보 검색 작업이 데이터 마이닝은 **아님** (ex. 데이터 베이스 시스템과 단순한 쿼리, 간단한 상호 작용)
- KDD(**데이터베이스 지식 발견**)의 필수적인 부분



#### **The process of KDD**

Input data -> Data Preprocessing -> Data Mining -> Postprocessing -> Information

- 데이터 전처리 : 원시 입력 데이터를 분석에 적합한 형식으로 변환 ( 기능 선택, 차원 축소, 정규화, 데이터 서브셋화), 가장 시간 많이 걸릴 수 있음

- 후처리 : 유용한 결과만이 의사결정을 지원 ( 필터링 패턴, 시각화, 패턴 해석 )
  - 사람의 해석





#### **Motivating Challenges**

데이터 마이닝을 어렵게 만드는 문제들

1. ##### 확장성

   - 데이터 마이닝 알고리즘이 **대량의** 데이터를 처리해야 함

   - 기술 : 기하급수적인 검색 문제를 처리하기 위한 특별한 검색 전략, 개별 레코드에 효율적으로 액세스하기 위한 새로운 데이터 구조, 아웃오브코어 알고리즘, 샘플링, 병렬 및 분산 알고리즘

   - ( 확장성 있는 알고리즘이란 데이터가 n배 늘었을 때 프로그램 수행시간이 n배보다 적거나 같은 경우)

2. ##### 고차원성 

   - 데이터 세트에는 **수천 개**의 속성 있는게 일반적

   - 문제 : 차원의 저주, 계산 복잡성 급속히 증가

3. ##### 이종 데이터

   - 데이터 세트에는 **다양한 유형**의 속성이 포함

   - 문제 : 데이터 마이닝 기술은 이러한 데이터 내의 관계를 고려해야 함

4. ##### 데이터 분산

   - 주요 과제 : 계산 실행에 필요한 통신량을 줄이는 방법, task를 여러위치로 나누고 각 위치에서 취득한 부분 결과를 병합하는 방법

   - ex. 하둡 : 사람이 프로그램 짜서 하둡에게 보내주면 알아서 10대의 컴퓨터에 일을 고르게 분산시켜 줌



#### **Disciplines Related to Data Mining**

- 데이터 마이닝은 다양한 분야의 기술을 채택
- **통계**(표본 추출, 추정, 가설검정), **인공지능**(기계학습, 모델링, 학습이론), **데이터베이스**(효율적인 저장소, 인덱싱, 쿼리 처리), **병렬/분산 컴퓨팅**(대규모 데이터 처리), 기타(최적화, 정보이론, 시각화, IR)
- +) 모델이란? 데이터를 단순하게 표현하는 것



#### **Data Science and Data Mining**

- data science : 데이터로부터 유용한 통찰력을 얻기 위한 도구와 기술을 연구하고 적용하는 학문 간 분야
  - 프로그래밍 스킬 + 수학/통계 스킬 + 도메인 스킬
  - +) 도메인 스킬 빠진게 데이터 마이닝

- data mining : 데이터에서 패턴과 관계를 직접 검출하는 것을 강조
  - 대부분의 경우 광범위한 도메인 지식 필요 없음



#### **Two Major Categories of Data Mining Tasks**

- **예측** (미래 예측, 다음 이벤트 예측) 
  - 다른 속성의 값을 기반으로 특정 속성의 값을 예측
  - ex. classification, regression, anomaly detection
- **기술** (데이터 묘사)
  - 데이터의 기본 관계를 요약하는 패턴을 유도하는 것
  - ex. (anomaly detection), association analysis, clustering



#### **Four Core Data Mining Tasks**

##### **1. Classification**

- 예측 모델링 : y = f(x)모델 구축 작업
  - y: 목표 변수 / x = (x1, x2, …, xn): 설명 변수
- 2종류의 예측 모델링
  - **분류** : y가 이산형인 경우 ( ex. 웹 사용자의 구매 여부 예측)
  - **회귀** : y가 연속형인 경우 ( ex. 지역의 미래 기온 예측)
- 목표 : 목표 변수의 예측값과 실제 값 사이의 오차를 **최소화**하는 모형 f(x) 학습
- 예시 : 꽃의 특성을 바탕으로 꽃의 종을 예측하는 작업

##### **2. Association Analysis**

- 데이터에서 강하게 **연관된** 항목을 설명하는 패턴 검색
  - ex. 함께 구입하는 제품군 찾기, 순차적으로 접속되는 웹 페이지 식별
- 발견된 패턴은 일반적으로 함축 규칙 또는 항목 하위 집합의 형태로 표현
  - ex. {기저귀} -> {맥주} : 함께 자주 구매
  - {우유, 햄, 빵} : 같이 등장한다.
- 목표 : 가장 **흥미로운** 패턴을 효율적인 방법으로 추출해야 한다. ( 데이터의 기하급수적인 크기 때문에)
- 예시 : 식료품점의 계산대에서 수집된 판매 데이터에서 관련 항목 간 교차 판매 기회에 사용 가능

##### **3. Cluster Analysis**

- **밀접하게 연관**된 관측치 그룹을 찾는다.
  - **similar** : 동일한 군집의 관측치는 서로 유사
  - **dissimilar** : 서로 다른 군집의 관측치는 서로 다름
- 적용 사례
  - 시장 조사 : 유사한 고객 그룹화 ( 고객 세분화 ) / 소셜 네트워크 : 대규모 그룹 내 커뮤니티 인식
- 예시 : 서류 클러스터링 ( 나오는 단어가 비슷한 기사끼리 묶어서 집단 만들기)

##### **4. Anomaly Detection**

- 특성이 나머지 데이터와 **유의하게** **다른** 관측치를 식별 (일반적인 데이터와 완전히 다른 데이터 찾기)
  - 이러한 관측치를 **anomalies** 또는 **ouliers** (이상치)
- 목표 : 실제 이상치를 찾아내고 정상적인 개체가 이상치로 판별하지 않도록 한다.
  - 높은 검출률, 낮은 오경보율( 이상치가 아닌 것을 이상치라고 하는 것을 낮추기)
- 적용 사례 : 네트워크 침입, 특이한 질병 패턴의 발견, 생태계 재난(가뭄, 홍수, 허리케인)
- +) 칼같이 outlier를 구별하는 기준은 없고 랭킹만 존재한다.
- 예시 : 신용카드 사기 탐지 ( 거래가 이전에 생성된 것들과 매우 다를 경우 잠재적으로 부정 행위라고 지정)





# ch2

#### **Terminology**

- 데이터 객체 : 측정 가능한 속성을 가진 엔티티 ( record(데이터베이스), point(벡터), sample, instance, observation )
- 속성 : 데이터 객체의 속성 또는 특성 ( variable(통계학), feature(인공지능), dimension, field )
- 데이터 셋 : 데이터 객체 집합



#### **Data-Related Issues of Data Mining**

1. ##### Types of data

   - 속성은 **다른** 유형일 수 있음 (ex. 범주형, 숫자형)

   - 데이터 집합의 특성이 **다른** 경우가 많음 (ex. 레코드 데이터, 그래프 데이터, 순서 데이터)

   - 데이터 유형에 따라 사용할 수 있는 도구와 기술이 결정됨

2. ##### Data quality

   - 데이터는 완벽과는 거리가 **멀다** (ex. 노이즈, 이상치, 누락 데이터, 불일치 데이터, 중복 데이터, 편향되거나 대표적이지 않은 데이터)

   - 일반적으로 데이터 품질을 이해하고 개선하면 결과 분석의 품질이 향상

3. ##### Preprocessing

   - raw데이터는 분석에 **적합하도록** 처리 되어야 함 (ex. 숫자형 속성_길이 -> 범주형 속성_S/M/L, 차원 축소(속성 값 줄이기))

   - 목표 : 특정 기술에 **잘 맞도록** 데이터를 수정하는 것

4. ##### Measures of similarity

   - 데이터 마이닝 작업에서는 객체 간의 **유사성**을 측정해야하는 경우 많음 (ex. clustering, classification, anomaly detection)

   - **많은** 유사성 측정 방법 존재



## Types of Data

#### **1. Types of Atrributes**

- **범주형**(질적) 속성
  - **제한된** 수의 가능한 값 중 하나를 취할 수 있는 속성
  - 숫자의 속성의 거의 없으므로 **기호**로 취급해야함
  - 값이 **순서**를 가질 수 있음 (ex. S < M < L )
- **수치**(양적) 속성
  - 값이 정의된 범위의 임의의 숫자일 수 있는 속성
  - 숫자의 속성이 대부분
  - **측정 척도**와 연관됨 (ex. 화씨, 섭씨, cm, kg, GB)

##### **Different Attribute Types**

범주형

순서x : 값은 단지 이름만 다름 ( 같다, 아니다)

순서o : 값은 객체를 정렬하기에 충분한 정보 제공 ( 크다, 작다)

숫자형 : 값은 숫자로 표시 ( 연산규칙)

##### **Another Way to Distinguish Attributes**

- **이산** 속성
  - **유한** 또는 **셀 수 있는** 무한대의 값 집합을 가짐
  - 범주형 (ex. 학생번호), 숫자형(ex. 티켓 숫자)
  - **정수** 변수를 사용해 표현
  - 이진속성 : 값이 두개 뿐인 특수한 경우
- **연속** 속성
  - 값이 실수인 것들, 일반적으로 **부동소수점** 변수로 표시





#### **2. Types of Data Sets**

\- 데이터 세트에는 여러가지 유형이 많지만 우리는 3개 볼 것임 ( 뒤에 있는 123 data들)

##### **General Characteristics of Data sets**

- 차수 
  - 데이터 세트의 속성의 수
  - **차원의 저주** : 고차원 데이터와 관련된 문제, 이로 인해 **차수 축소** 자주 사용
  - +) 고차원이면 분석이 복잡해지고 분석결과가 meaningless 해질 수 있음
- 분포
  - 속성으로부터 다양한 값의 빈도
  - but 많은 데이터 집합은 표준 통계 분포에 의해 **잘 포착되지 않은** 분포 가짐
  - 분산에서 **비대칭도**는 mining을 어렵게 만듬 (ex. 남자 : 여자 = 5 : 95)

##### **(1) Record Data**

- 데이터 세트는 **레코드**의 집합
- 레코드 또는 필드 간에 명확한 관계가 없음 ( 각각의 레코드는 독립적임)
- 보통 플랫 파일 또는 관계형 db에 저장
  - 그러나 데이터 마이닝은 관계형 db에서 사용할 수 있는 추가 정보를 사용하지 **않는** 경우가 많음
  - 오히려 db는 레코드를 찾는 편리한 장소 역할을 함
  - 레코드 데이타 예시 : Record data(각 컬럼당 값1개), Transaction data(한 컬럼에 값 여러개), Data matrix(모든 값 숫자), Document-term matrix

##### **(2) Graph-Based Data**

- 데이터는 1개 이상의 **그래프**로 표현
- (사례 1) 객체 간의 관계에 대한 데이터 : 그래프는 객체간의 관계를 포착함 (ex. sns, Linked web pages)
  - 노드 : 데이터 객체 / 링크 : 객체 간의 관계
- (사례 2) 그래프인 객체가 있는 데이터 : 데이터 객체 자체는 그래프로 표현됨 (ex. 화합물, 벤젠 분자)
- +) 그래프 데이터도 순서 있을 수 있음 (ex. 그래프가 시간에 따라 이러한 모양으로 바뀌고 있으니 이런 모양이 될 수 있다)

##### **(3) Ordered Data**

- 속성 값은 시간 또는 공간의 **순서**를 포함한다.
- (사례1) 순차적 거래 데이터 (ex. 매매거래 데이터, 구매기록)
  - 각 거래에는 그 거래와 관련된 타임스탬프가 있음
  - 순차적인 패턴을 찾을 수 있음 (ex. 장난감을 사는 사람은 배터리를 사는 경향이 있음)
- (사례2) 시계열 데이터 (ex. 센서 측정 데이터)
  - 각 레코드는 시계열임
  - 시간의 **자기 상관**을 고려하는 것이 중요 ( 바로 직전의 값과 많은 차이가 발생하지 않는 것, 시간이 가까운 두 값은 대부분 유사함)
- (사례3) 시퀀스 데이터 (ex. 단어의 배열, 유전자 배열 데이터)
  - 데이터 세트는 개별적인 엔티티의 시퀀스
  - 타임스탬프는 **없으나** 순서가 의미가 있음, 많은 문제는 유사한 시퀀스를 찾는 것과 관련이 있음
- (사례4) 공간 및 시공간 데이터 (ex. 지구 과학 데이터 집합, 가스류 데이터)
  - 데이터는 다양한 위치의 시계열로 구성됨
  - 보다 완전한 분석은 데이터의 공간적 측면과 시간적 측면을 모두 고려해야 함
  - **공간의 자기 상관**을 고려하는 것이 중요





## Data Quality

#### **Data Quality**

- 데이터가 완벽할 것이라는 기대는 **비현실적**
  - 인간 오류, 측정기기의 한계, 데이터 수집 프로세스의 결함
- 예시: 데이터 품질 문제
  - 값 또는 전체 데이터 객체가 누락될 수 있음, 복제된 개체 (한 사람 여러 레코드), 비일관성 (사람 키 2m지만 몸무게가 2kg)
- 데이터 마이닝은 데이터 품질 문제를 막기 위해
  1. 데이터 품질 문제 발견 및 시정 -> **데이터 클리닝** (데이터 전처리 하위개념)
  2. 데이터 품질 저하를 견딜 수 있는 알고리즘 사용



#### **Measurement and Data Collection Errors**

- 측정오차 : 측정 프로세스로 인한 문제 (ex. 측정값과 실제값의 오차)
- 데이터 수집 오류 : 데이터 개체 또는 속성 값을 누락하거나 데이터 개체를 부적절하게 포함하는 등의 오류 (ex. 유사하지만 관련이 없는 데이터 객체 포함)

#### **Measurement Error: Noise and Artifacts**

- Noise +)어쩔 수 없는 랜덤한 요소
  - 측정 오류의 **랜덤적**인 요소 : 일반적으로 값의 왜곡 또는 비논리적인 값의 추가가 포함
  - 제거가 어렵기에 많은 작업이 **강력한** 알고리즘에 집중됨 : 노이즈가 존재하는 경우에도 허용가능한 결과 얻을 수 있음
- Artifact +) 기계장치의 잘못
  - 데이터의 **결정론적** 왜곡 : ex. 한쌍의 사진 중 같은 장소의 줄무늬

#### **Measurement/Data Collection Error: Outliers**

- 데이터 집합에 포함된 대부분의 다른 데이터 개체와 **다른** 특성을 가진 데이터 개체
- 일반적인 값과 비교해 **특이한** 값
- **변칙적**인 물체 또는 값
- 노이즈와 이상치를 구별하는 것이 중요 : 이상치는 올바른 데이터 개체 또는 값일 수 있음
- +) 이상치는 칼같은 기준으로 나누지 못함

#### **Data Collection Error: Missing Values**

- 정보가 수집되지 않았거나 해당되지 않음
- 누락된 데이터에 대처하기 위한 몇가지 전략 :
  - 데이터 객체 또는 속성제거 (+ 없는 값을 무시, 없는 값의 열을 무시, 그 값을 채워넣기)
  - 결측값 추정 (ex. 평균, 보간법(앞 뒤의 값으로 중간 값 예측))
  - 분석 중 결측값 무시

#### **Data Collection Error: Inconsistent Values**

- 주어진 제약 조건을 **위반**하는 값 (+ 일관성 없는 데이터)
- 예시 : 같은 지역에 다른 우편 번호, 음의 몸무게, 존재하지 않는 이름, 6자리 전화번호
- 가능한 이런 문제를 발견해 수정하는 것이 중요 : 수정위해 **추가 or 외부 정보**가 필요할 수 있음

#### **Data Collection Error: Duplicate Data**

- 서로 **중복된** 데이터 개체
- 두가지 주요 이슈
  1. 실제로 단일 엔티티를 나타내는 두개의 값이 있는 경우 이러한 **일관되지 않은** 값을 해결하는 것이 중요
  2. 유사하지만 **다른** 물체가 잘못 결합되지 않도록 주의







## Data Preprocessing

- 데이터를 데이터 마이닝에 **더 적합**하게 만들기 위한 추가 단계
- 광범위한 분야로 복잡한 방식으로 상호 연관되어 있는 다양한 전략과 기술로 구성되어 있음
- 우리가 논의할 전략 : 집약, 샘플링, 차수 축소, 기능 선택, 기능 생성, 이산화 및 바이너리화, 변수 변환



#### **1. Aggregation**

\- 두개 이상의 개체를 하나의 개체로 결합 : 가끔은 적은 것이 더 나을 수 있음 (불필요한 data 삭제)

##### **Motivations for Aggregation**

1. 데이터 세트가 작을수록 메모리와 처리시간이 **감소** : 고가의 데이터 마이닝 알고리즘을 사용할 수 있음
2. 집약을 통해 데이터의 **개요**를 파악할 수 있음 (ex. 각 점포의 매출 -> 각 지역의 매출)
3. 객체 그룹의 행동이 개개의 객체 그룹보다 **더 안정적**임 (ex. 시간당 기온 -> 일평균 기온)

**단점** : 흥미로운 디테일이 손실될 수 있음 (ex. 몇개월 동안 집계 -> 매출이 가장 높은 날은? 못구하게됨)



#### **2. Sampling**

- 분석할 데이터 객체의 **부분 집합**을 선택
- 샘플링 동기 : 통계학자 ( 전체 데이터 집합을 얻는 데 너무 많은 비용이 듬), 데이터 마이너 ( 메모리 또는 처리 시간 측면에서 전체 데이터 세트를 **처리**하는데 너무 많은 비용이 듬)
- 효과적인 샘플링 핵심 원칙 : **대표 표본** 사용 ( 원래의 데이터 세트와 거의 동일한 속성 가져야 함)

##### **Sampling Approaches**

- 단순 랜덤 샘플링 (+ 완전히 임의 추출)
  - 특정 개체를 선택할 확률은 동일
  - 랜덤 표본 추출에 대한 두가지 변형 (복원 추출, 비복원 추출)
- 계층화된 샘플링 (+ 클래스의 비율만큼 뽑기)
  - 모집단의 개체 유형이 다른 경우 단순 무작위 표본 추출이 실패할 수 있음
  - **각** 그룹에서 개체 선택 ( 동일한 개체수, 해당 그룹에 크기에 비례하는 수)
- 점진적인(또는 적응형) 샘플링
  - 적절한 샘플 사이즈는 판단하기 어려울 수 있음
  - 작은 샘플부터 시작하여 충분한 크기의 샘플을 얻을 수 있을 때까지 샘플 크기를 늘림
  - 표본이 충분히 큰지 여부를 판단할 수 있는 방법이 있어야 함
  - +) 몇개를 뽑아야 충분할지 모르니 점진적으로 샘플을 계속 뽑음



#### **3. Dimensionality Reduction**

- 데이터 집합의 속성 수를 줄이는 프로세스 ( 차원 = 속성 수)
- 주요 장점
  1. 많은 데이터 마이닝 알고리즘은 차원성이 낮을수록 **더 잘** 작동 (부분적으로 관련 없는 기능이 제거, 노이즈가 감소, **차원의 저주**)
  2. 보다 이해하기 쉬운 모델 얻음 (모형에 포함되는 속성이 적기 때문에)
  3. 알고리즘에 필요한 시간과 메모리의 양이 줄어듬 (데이터의 크기가 줄어들기에)
  4. 데이터를 보다 쉽게 시각화 (데이터를 2, 3차원으로 줄일 수 있기 때문

##### **The Curse of Dimensionality**

- 데이터의 차원이 커짐에 따라 데이터 분석이 **현저하게 어려워지는** 현상
- 차원성이 높아짐에 따라 공간 내에서 데이터가 점점 **희박**해짐 ( 물체 사이의 거리가 **매우 커짐**, 결국 물체 사이의 거리는 **거의 같아짐**)
- **분류화** 문제 : 데이터 개체가 **충분하지 못하여** 가능한 모든 개체에 클래스를 안정적으로 할당하는 모델을 만들 수 없음
  - (+ 데이터 수는 고정되었으나 차원의 수가 폭발적으로 증가하면서 데이터의 빈공간이 커지면서 공간의 분류가 안됨)
- **클러스터링** 문제 : 객체 사이의 거리가 **덜 의미있게** 됨
  - (+ 유사한 거리에 있는 객체들을 모아야 하는 데 다 멀기에 클러스터링이 안됨)



#### **4. Feature Selection**

- 차수를 줄이는 또 다른 방법은 기능의 **부분집합**만 사용하는 것 (불필요하고 관계없는 속성을 제거함으로써)
- 중복 속성 : 다른 속성에 포함된 대부분 또는 전부의 정보를 복제한 것 (ex. 상품의 가격 <=> 판매세금의 양)
- 무관한 속성 : 데이터 마이닝 작업에 유용한 정보가 거의 없는 것 (ex. 학생 내신 예측 업무의 '학생증')

##### **Approaches to Feature Selection**

1. 상식 또는 도메인 지식을 사용
2. 내재된 접근 : 데이터 마이닝 알고리즘 **자체**가 사용할 속성을 결정 (ex. decision tree)
3. 필터 접근법 : 데이터 마이닝 알고리즘을 실행하기 전에 속성이 선택됨 (ex. 속성들 중 가능한 다른 것과 상관관계가 낮은 것을 선택, 상관관계가 높을수록 다른 한쪽 데이터는 무시해도 됨)
4. 래퍼 접근 : 타겟 데이터 마이닝 알고리즘을 블랙박스로 사용하여 **최적**의 속성 집합을 찾음 (ex. 퍼포먼스가 향상되는 한 속성 추가)
   - (+ 랩을 계속 감는 것처럼 속성을 계속 넣어봄)

##### **Feature Weighting**

- 중요도가 높은 기능을 더 중시하고 중요도가 낮은 기능을 더 낮게 할당
- 두가지 접근법
  - 속성의 상대적 중요성에 대한 도메인 지식 활용, 데이터 마이닝 알고리즘은 자동으로 가중치를 결정
- 예시) SVM : 각 속성에 가중치가 부여되는 분류모델 생성



#### **5. Feature Creation**

- 기존 속성에서 **새로운** 속성을 생성할 수 있는 경우가 빈번함 : 데이터 집합에서 중요한 정보를 훨씬 **효과적**으로 포착하기위해
- \- 예시) 우리는 역사적 유물을 그 재료에 따라 분류하길 원함 (ex. 목재, 점토, 청동, 금), 이 경우 질량 및 부피 속성으로 구성된 밀도 속성은 가장 직접적으로 정확한 분류를 산출함



#### **6. Discretization and Binarization**

- 이산화 : **연속** 속성을 **범주**형 속성으로 변환
- 이진화 : 속성을 하나 이상의 **이진** 속성으로 변환
- 이유 : 일부 데이터 마이닝 알고리즘에는 범주형 또는 이진 속성이 필요 (ex. 특정 분류 알고리즘, 클러스터링 알고리즘)

##### **Binarization**

범주형 값이 m개 있다고 가정 > 각 범주형 값에 대해 하나의 이진 속성 도입 > 각 m개의 이진속성에 대해 이진 속성이 개체의 범주 값을 나타내는 경우 1할당, 그렇지 않으면 0할당

##### **Discretization**

- 기본 절차
  - 몇개의 카테고리를 가질지 결정 > 연속 속상 값을 n개의 간격으로 나눔 > 한 간격의 모든 값을 동일한 범주형 값에 매핑
- 몇가지 간단한 접근법 : 동일한 넓이 이진화, 유사한 빈도 이진화, 클러스터링 기반 이진화

##### **Several Approaches to Discretization**

- 동일한 넓이 이진화: 범위를 **동일한 너비**를 가진 여러 개의 간격으로 나눔
- 유사한 빈도 이진화: 각 간격에 **동일한 개체 수**를 넣으려고 시도
- 클러스터링 기반 이진화: 개체의 클러스터를 찾아 **클러스터**에 따라 범위를 분할



#### **7. Variable Transformation**

- 변수의 모든 값에 변환 적용 (+ 값들을 함수에 적용해 특정 값들로 바꾸기)
- 종류1) 간단한 함수 : 각 값에 간단한 수학 함수를 개별적으로 적용 (ex. 로그 함수는 값이 매우 클 때 사용, 지수 로그 함수는 데이터를 정규 분포로 변환하는데 자주 사용)
- 종류2) 표준화 : 전체 값 집합이 특정 속성을 가지게 함 (ex. 통계의 변수 표준화), 값이 큰 변수가 분석 결과를 **지배하지 않도록** 하는 데 자주 사용





## Measures of Similarity

#### **Measures of Similarity and Dissimilarity**

- 객체 간의 유사성과 비유사성이 **중요함**
  - 많은 데이터 마이닝 기술에 의해 사용되기 때문 (ex.클러스터링, 가장 가까운 이웃 분류, 이상치 검출)
- **근접성** : 유사성 또는 비유사성 중 하나를 나타낼 때 사용
  - 물체에 대한 근접측정 방법 : 유클리드 거리, 자카드 계수, 코사인 유사도 등등
- 객체에 대한 **다양한 근접 방법**에 대해 논의

#### **Definitions**

- **유사성** : 두 물체가 비슷한 정도 (서로 닮을수록 유사성이 높아짐, 0덜유사 -> 1유사)
- **비유사성** : 두 물체가 다른 정도 ( 서로 유사할수록 비유사성은 낮아짐, 0유사 -> 1 또는 무한대 안유사)
  - +) distance라는 용어와 유사

#### **Transformations**

- 유사성은 비유사성으로 **변환**될 수 있음 (그 역에도 해당)
- 예시) 차이 그래프, 역수 그래프, 지수 그래프
- 일반적으로 단조로운 감소함소가 사용 가능

#### **Examples of Proximity Measures**

1. 민코우스키 거리 (맨해튼 거리, 유클리드 거리, 슈프리멈 거리)
2. 유사도 계수 (단순 매칭 계수, 자카드 계수)
3. 코사인 유사성
4. 상관관계

+) 2~4번은 유사성이 높을수록 점수가 높음, 1번은 유사할수록 점수 떨어짐



#### **1. Minkowski Distance**

- 거리 : 어떤 속성과의 비유사성 (+ dissimilarity의 하위집합)
- 유클리드 거리 : 두점 사이의 직선 거리
- 유클리드 거리의 일반화
- +) 맨허튼 유클리드 슈프리멈 적용되는 방법론에 따라 값이 달라짐

##### **The Properties of Distances**

- d(x, y)가 **거리**인 경우 다음 특성을 가짐 : **양수, 대칭성, 삼각 부등식**
- 이러한 특성은 거리에 대한 우리의 직관을 잘 표현하기 때문에 유용함



#### **2. Similarity Coefficients**

- 이진 속성만을 포함하는 개체간의 유사성 측정 : 일반적으로 0과 1사이의 값을 가짐
- 0 : 객체가 전혀 유사하지 않음 / 1 : 객체가 완전히 유사함
- 단순 매칭 계수 (SMC) : 존재 유무 모두 동등하게 카운트 (+ 같은 값 / 모든조합)
- 자카드 계수 (J) : 존재감만 계산 (+ 11 / 한 쪽이라도 일치하는 숫자가 나온 경우 10 01 11)



#### **3. Cosine Similarity**

- 두 벡터 x와 y사이의 (코사인)각도를 측정 (+ x y방향성이 완전히 일치하는 경우 : 1 / 정 반대인 경우 : -1 )
- **문서**간 유사도 측정에 유용
  - 문서는 종종 **벡터**로 표시 ( 각 구성요소는 특정용어의 빈도를 나타냄)
  - 0-0 매치는 무시됨 ( 양쪽 모두에 표시되지 않는 단어)
  - 0이 아닌 요소만 중요 (두가지 모두에 나타나는 단어)
- x와 y의 길이는 cos(x, y)에서 **중요하지 않음** (+ 각도를 따지기 때문)



#### **4. Correlation**

- 두 개의 집합 사이의 **선형 관계**를 측정
  - 1 : 양의 상관관계 / 0 : 상관 관계 없음 / -1 : 음의 상관관계
- 상관관계에는 여러종류가 있음 : 우리는 **피어슨 상관관계**에 초정

##### **Pearson's Correlation**

공분산(x, y) / 표준편차x * 표준편차y





#### **Mutual Information**

- **짝**이되는 값의 두 집합 간 유사성 측정 : 특히 **비선형** 관계가 의심될 경우
- 한 세트의 값이 다른 값에 대해 얼마나 많은 **정보**를 제공하는지 측정 (ex. 키와 몸무게)
- 두 값의 집합이 완전히 **독립**된 경우
  - 하나의 값은 다른 것에 대해 아무것도 설명하지 x, 상호정보는 0
- 두 값의 집합이 완전히 **종속**된 경우
  - 하나의 값을 아는 것은 다른 것의 값을 말해줄 수 있음, 최대한의 상호정보 얻을 수 있음

##### **Entropy**

- +) 값들이 모여있을 때 혼잡도 또는 정보량을 계산하는 방법
- 희귀한 값이 나오면 정보를 많이 얻음
- 유사한 것들이 많을수록 entropy 떨어짐
  - 즉 엔트로피(H(x))가 0이면 유사한 집단갯수가 많은거고 1이면 딱 절반씩 겁나 혼잡한 상황

##### **Definition: Mutual Information**

- 먼저 X, Y 및 (X, Y)의 **평균정보**를 측정 : 일반적으로 엔트로피에 의해 측정
- 즉 x가 y를 결정짓지 않을수록(그 역도 가능) 혼잡도가 높아짐
- X와 Y의 상호정보를 다음과 같이 취득
  - I(X, Y) = H(X) + H(Y) - H(X,Y)
    - 즉 X와 Y가 많이 겹칠 수록 복잡한 상황 (뭐가 나올지 모르는 상황)이니까 I(얻을 수 있는 정보)가 커진다.
  - 상호정보는 Y를 관측하면서 X에 대해 얻은 '정보량'을 정량화한다 ( 역도 가능)
  - 즉 I(X, Y) = I(Y, X)
- 예시) y = x^2 은 상관관계는 0이지만 상호정보는 1.9502임





#### **Issues in Proximity Calculation**

- (문제 1) : 표준화
  - 속성의 척도가 다른 경우 값이 큰 속성의 지배를 받지 않도록 표준화 함
  - 재설계 : 속성 범위를 [0, 1]로 다시 조정
  - 평균 정규화 : 평균으로부터의 거리에 따라 척도 변경
  - 표준화(통계) : 0 평균 1분산을 설정함
- (문제 2) : 가중치 사용
  - 경우에 따라서 일부 속성이 다른 속성보다 중요함 (ex. 두 음료 비교시 색깔보다 맛이 더 중요함)
  - 각 속성에 다른 가중치를 할당할 수 있음











﻿
