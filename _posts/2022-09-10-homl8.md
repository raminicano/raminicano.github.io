---
layout: single
title: "차원 축소"
categories: Hands-On-ML
tag: [ML]
toc: true
---











# 들어가며

- 차원축소는 데이터 시각화에도 아주 유용하다.
  - 고차원 훈련세트를 하나의 압축된 그래프로 그릴 수 있다.
  - 군집 같은 시각적인 패턴을 감지해 중요한 통찰을 얻는다.





# 차원의 저주

- 훈련세트의 차원이 클수록 과대적합 위험이 커짐
- 해결책
  - 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련세트의 크기를 키우는 것
    - but, 필요한 훈렴샘플 수는 차원 수가 커짐에 따라 기하급수적으로 늘어나 힘들다.





# 차원 축소를 위한 접근 방법

## 투영

- 대부분의 실전 문제는 훈렴 샘플이 모든 차원에 걸쳐 균일하게 퍼져있지 않음
  - 대부분의 특성은 거의 변화가 없고 몇몇의 특성들이 서로 강하게 연관되어 있음
- 즉, 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간**(subspace) 에 놓여 있음
  - ![image](https://user-images.githubusercontent.com/97875918/189511977-b0975984-5ce7-46c9-a719-e6ecc6a27193.png) 
  - 모든 훈련 샘플이 거의 평면 형태임
    - 고차원(3D) 공간에 있는 저차원(2D) 부분 공간이다.
  - 모든 훈련 샘플을 부분 공간에 수직으로 투영
    - ![image](https://user-images.githubusercontent.com/97875918/189512041-547a53a9-b5a6-4af3-a5af-93bc04f56066.png)
    - 3D 데이터를 2D로 줄임
    - 각 축은 새로운 특성 z1, z2에 대응됨
- but, 투영이 언제나 최선의 방법은 아니다.
  - ![image](https://user-images.githubusercontent.com/97875918/189512488-b5aba591-ee19-414b-80c7-1b5c3fc2f15d.png)
  - **스위스 롤**(Swiss roll) 데이터 셋처럼 부분공간이 뒤틀리거나 휘어있는 경우
  -  ![image](https://user-images.githubusercontent.com/97875918/189512498-74565b54-b9e4-44dc-94aa-004ec9dc5e5e.png)
    - 왼쪽  : 평면에 투영시켜 층이 서로 뭉개짐
    - 오른쪽 : 스위스 롤을 펼쳐서 잘 분리됨





## 매니폴드 학습

- 2D **매니폴드**

  - ex) 스위스 롤
  - 고차원 공간에서 휘어지거나 뒤틀린 2D 모양

- d차원 매니폴드는 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부이다. (d < n)

  - 스위스롤의 경우 d=2, n=3

- **매니폴드 학습** (manifold learning)

  - 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링하는 식으로 작동함

  - **매니폴드 가정** 또는 **매니폴드 가설**에 근거함

    - 대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여있다.

  - | ![image](https://user-images.githubusercontent.com/97875918/189512930-a13949ec-b61b-47e2-8084-bf715f63a9cb.png) | ![image](https://user-images.githubusercontent.com/97875918/189512935-ad90dba5-24ca-443d-8a5e-e607cf34fed9.png) |
    | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | 3D 데이터                                                    | 결정경계를 이용해 두개 클래스로 간단히 나눔                  |
    | ![image](https://user-images.githubusercontent.com/97875918/189512820-8fbdd251-7e31-4bba-aa60-6719b9f936d0.png) | ![image](https://user-images.githubusercontent.com/97875918/189512829-56499af5-252e-4c48-92fe-3920b007b6aa.png) |
    | 결정경계가 x1=5에 놓여 매우 단순함                           | 오히려 펼치니 결정경계가 더 복잡함                           |

  - 요약

    - 모델을 훈련시키기 전에 훈련 세트의 차원을 감소시키면 훈련 속도는 빨라지지만 항상 더 낫거나 간단한 솔루션이 되는 것은 아니다.
      - 전적으로 데이터셋에 달림









# PCA

- **주성분 분석** (principal component analysis, PCA)
  - 데이터에 가장 가까운 초평면 (hyperplane)을 정의한 다음 데이터를 이 평면에 투영시킴





## 분산 보존

- 저차원의 초평면에 훈련 세트를 투영하기 위해 올바른 초평면을 선택해야 함
- ![image](https://user-images.githubusercontent.com/97875918/189513714-77836120-456c-4b9c-bc3f-33d3502db8c6.png)
  - 간단한 2D 데이터셋을 직선과 각 축으로 투영된 결과
  - 실선에 투영된 것이 분산을 최대로 보존함
- 분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되어 합리적임
  - 즉, 원본데이터셋과 투영하려하는 축 사이의 평균제곱 거리를 최소화하는 것





## 주성분

- PCA에서 주성분을 찾는 과정

  - 훈련 세트에서 분산이 최대인 축을 찾음
  - 첫번째 축에 직교하고 분산을 최대한 보존하는 남은 두번째 축을 찾음
  - 이전의 두축에 직교하는 세번째 축을 찾음
  - 데이터 셋에 있는 차원의 수만큼 4,5,...n번째 축을 찾는다.

- i번째 축을 데이터셋의 i번째 **주성분** (PC) 라고 부른다.

- 훈련 세트의 주성분을 찾는 기술

  - 특이값 분해 (Singular Value Decomposition, SVD)라는 표준 행렬 분해 기술 사용

  - ```python
    X_centered = X - X.mean(axis=0)
    # 훈련세트의 모든 주성분을 구함
    U, s, Vt = np.linalg.svd(X_centered)
    # 처음 2개의 PC를 정의하는 두개의 단위벡터 추출
    c1 = Vt.T[:, 0]
    c2 = Vt.T[:, 1]
    
    # d차원으로 투영하기
    W2 = Vt.T[:, :2] # 첫 두개의 주성분으로 정의된 평면에 훈련 세트를 투영
    X2D = X_centered.dot(W2)
    ```

    - PCA는 데이터셋의 평균이 0이라 가정함
      - 사이킷런의 PCA는 이 작업을 대신 처리해줌
      - 위 코드처럼 직접 PCA를 구현하거나 다른 라이브러리를 사용시 먼저 데이터를 원점에 맞춰야 함







## 사이킷런 사용하기

- 사이킷런의 PCA 모델은 앞서 한 것 처럼 SVD 분해 방법을 사용해 구현함

- PCA 모델을 사용해 데이터셋의 차원을 2로 줄이는 코드

  - ```python
    from sklearn.decomposition import PCA
    
    pca = PCA(n_components=2)
    X2D = pca.fit_transform(X)
    # 첫번째 주성분을 정의하는 단위벡터
    pca.components_.T[:,0]
    ```

- **설명된 분산의 비율**

  - 각 주성분의 축을 따라 있는 데이터셋의 분산비율을 나타냄

  - ```python
    pca.explained_variance_ratio_
    # array([0.84248607, 0.14631839])
    ```

    - 데이터셋 분산의 84.2%가 첫번째 PC를 따라 놓여있고 14.6%가 두번째 PC를 따라 놓여 있음
    - 2D로 투영했기에 분산의 1.1%를 잃었음

- **적절한 차원 수 선택하기**

  - 축소할 차원 수를 임의로 정하는 것보다 충분한 분산(ex. 95%)이 될때까지 더해야할 차원 수를 선택할 수 있음

    - ```python
      # pca를 계산한 뒤 훈련세트의 분산을 95%로 유지하는 데 필요한 최소한의 차원수를 계산
      pca = PCA() # n_components 지정하지 않으면 특성수와 샘플 수 중 작은 값으로 설정
      pca.fit(X_train)
      cumsum = np.cumsum(pca.explained_variance_ratio_) # 입력 배열의 원소를 차례대로 누적한 배열 반환
      d = np.argmax(cumsum >= 0.95) + 1
      
      # 보존하려는 분산의 비율을 지정해서 설정
      pca = PCA(n_components=0.95)
      X_reduced = pca.fit_transform(X_train)
      pca.n_components_
      ```

  - 설명된 분산을 차원수에 대한 함수로 그리는 방법 (cumsum변수를 그래프로 그림)

    - ![image](https://user-images.githubusercontent.com/97875918/189517562-72278f1b-dcbf-4c75-aa87-1ef965670db1.png)
    - 설명된 분산의 빠른 성장이 멈추는 변곡점이 존재함
    - 이 경우 차원을 100으로 축소해도 설명된 분산을 크게 손해보지 않음

  - 데이터 시각화를 위해서 차원을 축소하는 경우 차원을 2-3개로 줄이는 것이 일반적

- **압축을 위한 PCA**

  - 차원을 축소하고 난 후는 훈련세트의 크기가 줄어들음

  - 압축된 PCA 투영의 변환을 반대로 적용해 다시 되돌릴 수 있음

    - 투영에서 일정량의 정보를 잃어버렸기에 원본 데이터셋을 얻을 수 없음 (그러나 매우 유사)

    - **재구성 오차** (reconstruction error)

      - 원본데이터와 재구성된 데이터 사이의 평균 제곱거리

    - ```python
      # 압축
      pca = PCA(n_components=154)
      X_reduced = pca.fit_transform(X_train)
      # 재구성
      X_recovered = pca.inverse_transform(X_reduced)
      ```







### 랜덤 PCA

- svd_solver 매개변수를 "randomized"로 지정하면 사이킷런은 랜덤 PCA라 부르는 확률적 알고리즘을 사용해 처음 d개의 주성분에 대한 근삿값을 빠르게 찾음

  - svd_solver의 기본값은 "auto"
    - m이나 n이 500보다 크고 d가 m이나 n의 80%보다 작으면 사이킷런은 자동으로 랜덤 PCA 알고리즘을 사용
    - 아니면 완전한 SVD 방식
      - 강제하려면 "full"로 지정

- 계산복잡도

  - O(m*d^2) + O(d^3)
  - 완전한 SVD 방식은 O(m*n^2) + O(n^3)
  - d가 n보다 많이 작으면 완전 SVD보다 훨씬 빠름

- ```python
  rnd_pca = PCA(n_components=154, svd_solver="randomized", random_state=42)
  X_reduced = rnd_pca.fit_transform(X_train)
  ```





### 점진적 PCA

- PCA 구현의 문제

  - SVD 알고리즘을 실행하기 위해 전체 훈련 세트를 메모리에 올려야 한다는 것

- 점진적 PCA (IPCA)

  - 훈련세트를 미니배치로 나눈 뒤 IPCA알고리즘에 한번에 하나씩 주입
  - 훈련세트가 클 때 유용
  - 온라인으로 PCA적용할 수 있음

- ```python
  from sklearn.decomposition import IncrementalPCA
  
  n_batches = 100
  inc_pca = IncrementalPCA(n_components=154)
  for X_batch in np.array_split(X_train, n_batches): # 100개의 미니배치로 나눔
      inc_pca.partial_fit(X_batch) # fit()은 전체 훈련세트를 사용함
  
  X_reduced = inc_pca.transform(X_train)
  ```













# 커널 PCA











# LLE











# 다른 차원 축소 기법











