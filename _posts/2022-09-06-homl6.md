---
layout: single
title: "결정 트리"
categories: Hands-On-ML
tag: [ML]
toc: true
---





# 결정 트리 학습과 시각화

- 분류, 회귀, 다중 출력 작업 가능한 머신러닝 알고리즘

- 매우 복잡한 데이터셋도 학습할 수 있음

- 결정트리는 가장 강력한 머신러닝 알고리즘 중 하나인 랜덤포레스트의 기본 구성요소

- 사이킷런은 이진트리만 만드는 CART 알고리즘을 사용

  - 리프 노드 외의 모든 노드는 자식 노드를 두개씩 가짐

  - but ID3 같은 알고리즘은 둘 이상의 자식노드를 가진 결정트리 생성 가능

- 장점

  - 데이터 전처리가 거의 필요하지 않음
  - 특성의 스케일을 맞추거나 평균을 원점에 맞추는 작업이 필요하지 않음

- 계산 복잡도

  - 예측을 위해 루트노드에서부터 리프 노드까지 탐색해야 함
    - 예측을 할 시 약 O(log2(m))개의 노드 거쳐야 함
    - 큰 훈련 세트를 다룰 때에도 예측 속도가 매우 빠름

  - 훈련 알고리즘은 각 노드에서 모든 훈련 샘플의 모든 특성을 비교 (max_features가 지정되면 그보다는 적게)
    - 모든 샘플 모든 특성 비교 시 O(n*mlog2(m))이 됨

- 붓꽃 데이터셋에 DecisionTreeClassifier를 훈련시킴

  - ![image](https://user-images.githubusercontent.com/97875918/188482021-b31b5299-b34c-4eda-ab95-32f3ca597326.png)

    







# 예측하기

- 위의 그림 참고해 새로 발견한 붓꽃의 품종을 분류하려 한다고 가정
- 루트 노드에서 시작
  - 꽃잎의 길이 2.45cm보다 짧은지 검사
  - 맞으면 왼쪽의 자식 노드로 이동
- 리프노드의 경우 추가적인 검사를 하지 않음
- 노드에 있는 예측 클래스를 보고 결정 트리가 새로 발견한 꽃의 품종을 예측함
- sample 속성
  - 얼마나 많은 훈련 샘플이 적용되었는지 확인 가능

- value 속성
  - 각 클래스에 얼마나 많은 훈련 샘플이 있는지 알려줌

- gini 속성
  - 불순도를 측정
  - 한 노드의 모든 샘플이 같은 클래스에 속해있다면 이 노드를 순수(gini=0)하다고 함

- ![image](https://user-images.githubusercontent.com/97875918/188650228-c73b4c8a-21e5-482e-b9fe-ba8cc5deb8a8.png)
  - 굵은 수직선 : 루트 노드의 결정 경계
    - 왼쪽 영역은 순수노드만 있기에 더이상 나눌 수 없음
    - 오른쪽 영역은 max_depth를 2로 설정했기에 더 분할x
      - max_depth가 3이면 점선을 기준으로 나눈다.

  - 화이트박스 모델
    - 직관적이고 결정 방식을 이해하기 쉬운 모델
    - ex) 결정트리

  - 블랙박스 모델
    - 성능이 뛰어나고 예측을 만드는 연산과정을 쉽게 확인
    - but 왜 그런 예측을 만드는지 쉽게 설명하기 어려움
    - ex) 랜덤포레스트, 신경망








# 클래스 확률 추정

- 결정트리는 한 샘플이 특정 클래스 k에 속할 확률을 추정할 수 있다.
  - 샘플에 대한 리프 노드를 찾기 위해 트리를 탐색함
  - 그 노드에 있는 클래스 k의 훈련 샘플의 비율을 반환
    - 클래스가 k개 있다면 k개에 대한 확률을 반환







# CART 훈련 알고리즘

- 사이킷런은 결정 트리를 훈련시키기 위해(트리의 성장을 위해) CART(Classification And Regression Tree) 알고리즘을 사용함
- 훈련세트를 하나의 특성 k의 임곗값 t를 사용해 두개의 서브셋으로 나눔
  - ex) 꽃잎의 길이 <= 2.45cm
- k와 t를 고르는 방법
  - 가장 순수한 서브셋으로 나눌 수 있는 (k, t)짝을 찾음
  - 이 알고리즘이 최소화해야 하는 비용함수
    - ![image](https://user-images.githubusercontent.com/97875918/188654389-17561635-fbd0-4320-b6cf-e5b2e1ba8805.png)
  - 계속 서브셋을 나누다가 최대 깊이(매개변수 max_depth)가 되면 중지함
  - 불순도를 줄이는 분할을 찾을 수 없을 때 중지함
- 탐욕적 알고리즘(greedy algorithm)
  - 맨 위 루트 노드에서 최적의 분할을 찾으며 이어지는 각 단계에서 서브셋으로 나누는 과정을 반복함
  - 현재 단계의 분할이 몇 단계를 거쳐(미래) 가장 낮은 불순도로 이어질 수 있을지 없을지는 고려하지x
  - 훌륭한 솔루션을 만들지만 최적의 솔루션을 보장하지는 x
    - 최적의 트리는 NP-완전 문제와 연관되어 있어 납득할만한 솔루션으로만 만족해야 함









# 지니 불순도 또는 엔트로피?

- 기본적으로 지니 불순도 사용 됨
  - DecisionTreeClassifier의 criterion 의 기본 값은 "gini"
  - DecisionTreeRegressor의 기본값은 "mse"
- criterion 매개변수를 "entropy"로 지정해 엔트로피 불순도를 사용할 수 있음
- **엔트로피**
  - 분자의 무질서함을 측정하는 것 (열역학의 개념)
  - 분자가 안정되고 질서 정연하면 엔트로피는 0에 가까움
  - 머신러닝에서는 불순도의 측정방법으로 자주 사용
    - 어떤 세트가 한 클래스의 샘플만 담고 잇다면 엔트로피가 0임
- 지니 불순도 vs 엔트로피
  - 실제로 큰 차이 없음
  - 지니 불순도가 조금 더 계산이 빨라 기본값으로 좋음
  - 비슷한 트리를 만들어내지만 다른 트리가 만들어진 경우
    - 지니 불순도는 가장 빈도가 높은 클래스를 한쪽 가지(branch)로 고립시키는 경향
    - 엔트로피는 좀 더 균형잡힌 트리 만든다.













# 규제 매개변수

- 결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없음
  - 선형 모델의 경우는 데이터가 선형일 거라고 가정함
  - 제한을 두지 않으면 과대적합되기 쉬움
- 비파라미터 모델(nonparametric model)
  - 모델 파라미터가 없는 것이 아니라 훈련되기 전에 파라미터 수가 결정되지 않는 모델
  - 모델 구조가 데이터에 맞추어 고정되지 않고 자유로움
  - ex) 결정트리
- 파라미터 모델(parametric model)
  - 미리 정의된 모델 파라미터 수를 가짐
  - 자유도가 제한되고 과대적합될 위험이 줄어든다. (과소적합 위험은 커짐)
  - ex) 선형모델
- 결정 트리의 자유도를 제한할 규제 매개변수
  - 사이킷런의 max_depth
    - 기본값은 제한이 없는 None 으로 설정됨
  - DecisionTreeClassifier에 존재하는 매개변수
    - min_samples_split : 분할되기 위해 노드가 가져야 하는 최소 샘플 수
    - min_samples_leaf : 리프 노드가 가지고 있어야 할 최소 샘플 수
    - min_weight_fraction_leaf : min_samples_split 와 같지만 가중치가 부여된 전체 샘플 수 에서의 비율
    - max_leaf_nodes : 리프 노드의 최대 수
    - max_features : 각 노드에서 분할에 사용할 특성의 최대 수
  - ![image](https://user-images.githubusercontent.com/97875918/188664328-6684792c-0a31-42db-8c07-444e26ca61ce.png)
    - 왼쪽 트리
      - 기본 매개변수를 사용해 훈련 (규제x)
    - 오른쪽 트리
      - 매개변수 min_samples_leaf =4 지정해 훈련











# 회귀

- 결정트리는 회귀문제에도 사용 가능
- DecisionTreeRegressor를 사용해 잡음이 섞인 2차 함수 현태의 데이터셋에서 max_depth=2로 회귀 트리 생성
  - ![image](https://user-images.githubusercontent.com/97875918/188672806-0bb61e48-178c-483e-b2a1-f88dbfad1b1a.png)
  - 분류와 차이점
    - 클래스를 예측하는 대신 어떤 값을 예측함
    - ex) x=0.6인 샘플의 타깃값 예측
      - value=0.111인 리프노드에 도달
      - 이 값은 리프노드에 있는 110개의 훈련 샘플의 평균 타깃값임
  - ![image](https://user-images.githubusercontent.com/97875918/188673410-26945325-1efd-4cb9-a74e-7d23aabadfb5.png)
    - 각 영역의 예측값은 그 영역에 있는 타깃값의 평균임
    - 알고리즘은 예측값과 가능한 한 많은 샘플이 가까이 있도록 영역을 분할함
- CART 알고리즘
  - 분류와 차이
    - 훈련 세트를 불순도를 최소화하는 방향으로 분할하는 대신 MSE(평균제곱오차)를 최소화하도록 분할한다.









# 불안정성

- 결정트리는 계단 모양의 결정 경계를 만든다.
  - 모든 분할은 축에 수직임
  - 훈련세트 회전에 민감함
    - ![image](https://user-images.githubusercontent.com/97875918/188665041-6abcfd29-fe5e-45ee-b71a-04bfb888b7ac.png)
      - 데이터의 회전에 따라 결정경계가 바뀌는 모습
      - 왼쪽 결정트리
        - 쉽게 데이터셋을 구분함
        - 일반화 잘됨
      - 오른쪽 결정트리
        - 불필요한 구불구불
        - 일반화 잘되지 못함
      - 이런 문제 해결방법
        - 훈련 데이터를 더 좋은 방향으로 회전시키는 PCA 기법 활용
  - 훈련 데이터에 있는 작은 변화에도 아주 민감함
    - 특정 데이터를 제거하고 결정 트리를 훈련시키면 아주 다른 모델이 나올 수 있음
  - 훈련 알고리즘이 확률적이기 때문에 같은 훈련 데이터에서도 다른 모델 얻을 수 있음
    - random_state로 고정 가능









