---
layout: single
title: "서포트 벡터 머신"
categories: Hands-On-ML
tag: [ML]
toc: true
---









- 서포트 벡터 머신 (Support Vector Machine)은 매우 강력하고 선형, 비선형 분류, 회귀, 이상치 탐색에도 사용할 수 있는 다목적 머신러닝 모델
- 복잡한 분류 문제 잘 맞으며 작거나 중간 크기의 데이터셋에 적합함

# 선형 SVM 분류

1. **기본 아이디어**

   - ![image](https://user-images.githubusercontent.com/97875918/188302225-b8aa15b7-4c85-43fb-82b9-74c9fefc715c.png)
   - 왼쪽 그래프
     - 분류기 : 선형분류
     - 점선으로 나타난 결정경계를 만든 모델은 클래스를 적절하게 분류하지 못함
     - 나머지 직선 모델은 훈련세트에 대해 완벽하게 동작
     - but 결정경계가 샘플에 너무가까워 새로운 샘플에 대해서는 잘 작동x
   - 오른쪽 그래프
     - 분류기 : 마진분류
     - 실선은 SVM분류기의 결정 경계임
     - 두개의 클래스를 잘 나눔 + 훈련 샘플로부터 멀리 떨어짐
     - 도로 경계에 위치한 샘플에 의해 전적으로 결정됨
       - 이런 샘플을 **서포트 벡터** (support vector)
   - SVM 분류기는 클래스 사이에 가장 폭이 넓은 도로를 찾는 것으로 생각할 수 있음
     - **라지 마진 분류** (large margin classification)
   - SVM은 스케일에 민감함
     - ![image](https://user-images.githubusercontent.com/97875918/188302410-910e38ee-4894-4f9c-a08c-0680cb77e41d.png)
     - 왼쪽 그래프
       - 수직축과 수평축의 스케일이 상이해 도로가 거의 수평임
     - 오른쪽 그래프
       - 도로 훨씬 넓어짐 (good)

2. **소프트 마진 분류**

   - 하드 마진 분류

     - 모든 샘플이 도로 바같쪽에 올바르게 분류되어 있음

     - 문제점

       - 데이터가 선형적으로 구분될 수 있어야 제대로 작동함
       - 이상치에 민감함
       - ![image](https://user-images.githubusercontent.com/97875918/188302858-85968d48-5101-4962-94e3-56e87120f896.png)
         - 왼쪽 그래프
           - 하드마진을 찾을 수 없음
         - 오른쪽 그래프
           - 일반화가 잘 되지 않음 (도로의 폭이 좁음)

     - 소프트 마진 분류

       - 하드마진 보다 더 유연한 모델

         - 도로의 폭을 가능한 넓게 유지함
         - **마진 오류**(샘플이 도로 중간이나 반대쪽에 있는 경우) 사이에 적절한 균형을 잡음

       - 사이킷런의 SVM모델 생성 파라미터 중 C

         - ![image](https://user-images.githubusercontent.com/97875918/188303059-8aa11de2-19c7-43c3-9ff9-af414f3ae500.png)
         - 왼쪽 그래프
           - 마진 오류가 많으나 일반화가 잘 됨
         - 오른쪽 그래프
           - 마진 오류는 적으나 일반화가 잘되지 못함
         - SVM 모델이 과대적합 이라면 C를 감소시켜 모델을 규제함

       - SVM 분류기는 로지스틱 회귀 분류기와 다르게 클래스에 대한 확률을 제공하지 않음

         - but probability=True로 매개변수를 지정하면 predict_proba() 메서드를 제공

       - ```python
         from sklearn.pipeline import Pipeline
         from sklearn.preprocessing import StandardScaler
         from sklearn.svm import LinearSVC
         
         iris = datasets.load_iris()
         X = iris["data"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 너비
         y = (iris["target"] == 2).astype(np.float64)  # Iris virginica
         
         svm_clf = Pipeline([
                 ("scaler", StandardScaler()), #스케일링
                 ("linear_svc", LinearSVC(C=1, loss="hinge", random_state=42)),
             ])
         
         svm_clf.fit(X, y)
         # 예측
         svm_clf.predict([[5.5, 1.7]])
         ```

3. 사이킷 런의 선형 SVM 지원 모델

   - ```python
     # LinearSVC 위에 있음
     # SVC + 선형커널
     SVC(kernel="linear", C=1)
     # SGDClassifier + hinge 손실함수 활용 + 규제
     # LinearSVC보다 빠르게 수렴하지는 않지만 데이터셋이 아주 커서 메모리에 적재할 수 없거나 온라인 학습으로 분류 문제를 다룰 때 유용
     SGDClassifier(loss="hinge", alpha=1/(m*C)) #규제강도가 훈련샘플수(m)에 반비례
     ```







# 비선형 SVM 분류

- 선형적으로 분류할 수 없는 데이터셋이 많음



- 비선형 데이터셋을 다루는 방법
  1. **선형SVM 적용**
     1. 다항 특성같은 특성 추가한 후 선형 SVM 적용
        - ![image](https://user-images.githubusercontent.com/97875918/188303614-592dea5a-0005-43e4-b4cd-e9982ae2e4d2.png)
        - 왼쪽 그래프
          - 하나의 특성만을 가짐
          - 선형적으로 구분이 안됨
        - 오른쪽 그래프
          - 다항 특성을 추가함
          - 선형적으로 구분 가능
     2. 유사도 특성 활용
        - 유사도 특성을 추가 or 유사도 특성만을 활용해 선형 SVM 적용
  2. **SVC + 커널트릭**
     - 새로운 특성을 실제로 추가하지는 않지만 동일한 결과를 유도하는 방식
     - ex) 다항 커널
     - ex) 가우시안 RBF 커널







1. **다항식 커널**

   - 다항 특성을 추가하는 효과를 내주는 함수

   - **커널 트릭** (kernel trick)

     - 실제로 특성을 추가하지 않으면서 다항식 특성을 많이 추가한 것과 같은 결과를 얻을 수 있음

     - ```python
       from sklearn.svm import SVC
       
       poly_kernel_svm_clf = Pipeline([
               ("scaler", StandardScaler()),
               ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5)) # coef0은 모델이 높은 차수와 낮은 차수에 얼마나 영향을 받을지 조절, 3차 다항식 커널을 사용해 SVM 분류기를 훈련시킴
           ])
       poly_kernel_svm_clf.fit(X, y)
       ```

2. **유사도 특성**

   - 비선형 특성을 다루는 또다른 기법으로 유사도 함수로 계산한 특성을 추가
   - **유사도 함수** (similarity function)
     - 각 샘플이 특정 **랜드마크**와 얼마나 닮았는지 측정하는 함수
     - 예시) 1차원 데이터셋에 두개의 랜드마크 (x1 = -2, x1 = 1)을 추가, 감마=0.3인 **가우시안 방사기저 함수**(RBF)를 유사도 함수로 정의
       - ![image](https://user-images.githubusercontent.com/97875918/188304619-2b1c8973-7636-404a-8cef-a061578d9641.png)
       - RBF의 값은 0(랜드마크에서 아주 멀리 떨어진 경우)부터 1(랜드마크와 같은 위치)까지 변화하며 종모양으로 나타남
         - 감마는 0보다 커야하며 값이 작을수록 폭이 넓은 종모양임
       - 샘플 x1 = -1이 있다고 하면 새로만든 특성은 x2(첫번째 랜드마크)는 약 0.74, x3(두번째 랜드마크)는 약 0.3임
       - ![image](https://user-images.githubusercontent.com/97875918/188304740-2aef1d96-15d8-4a23-be55-8faaf48464b0.png)
         - 왼쪽그래프는 선형으로 구분불가능 했는데 RBF를 유사도 함수로 정의하여 새로운 특성을 만들어 오른쪽 그래프처럼 선형적으로 구분 가능하게 되었음
       - 랜드마크 선택 방법
         - 데이터셋에 있는 모든 샘플 위치에 랜드마크를 설정
           - 장점 : 차원이 매우 커지고 변환된 훈련세트가 선형적으로 구분될 가능성이 높음
           - 단점 : 훈련 세트에 있는 n개의 특성을 가진 m개의 샘플이 m개의 특성을 가진 m개의 샘플로 변환됨
             - 훈련세트가 매우 클 경우 동일한 크기의 아주 많은 특성이 만들어짐

3. **가우시안 RBF 커널**

   - 유사도 특성을 추가하는 효과를 내주는 함수
   - RBF 커널을 사용한 SVM 분류기
     - ![image](https://user-images.githubusercontent.com/97875918/188305179-101dfeb3-edec-40b9-bafd-68a26f9abb72.png)
     - 감마 증가 시 종모양 그래프가 좁아져 각 샘플의 영향 범위가 작아짐
       - 결정 경계가 더 불규칙해지고 각 샘플을 따라 구불구불하게 휘어짐
     - 감마 감소 시 종모양 그래프가 넓어져 각 샘플의 영향 범위가 커짐
       - 결정 경계가 더 부드러워짐
   - 하이퍼파라미터 감마가 규제의 역할을 한다.
     - 감마 값이 커질수록 랜드마크에 많이 집중함
     - 모델 과대적합 : 감소시킴
     - 과소적합 : 증가시킴
     - 하이퍼파라미터 C와 유사
       - C의 값이 커질수록 가중치 규제를 적게한다.
   - 다른 커널
     - 거의 사용되지 않음
     - 문자열 커널, 문자열 서브시퀀스 커널, 레벤슈타인 거리 기반의 커널
       - 텍스트 문서나 DNA 서열을 분류할 때 사용
   - 추천 커널
     - 선형 커널을 가장 먼저 시도해봐야 함
       - 가장 빠르다.
       - 선형 모델이 에상되는 경우
       - 특히 훈련세트가 아주 크거나 특성수가 많을 경우
     - 가우시안 RBF 커널
       - 훈련세트가 너무 크지 않은 경우
       - 대부분의 경우 잘 들어맞음
     - 시간과 컴퓨팅 성능이 충분하다면 교차 검증과 그리드 탐색을 사용해 다른 커널도 시도해 볼 수 있음
       - 훈련 데이터의 구조에 특화된 커널이 있다면 테스트 해보기

4. **계산 복잡도**

   - LinearSVC 파이썬 클래스는 선형 SVM을 위한 최적화된 알고리즘을 구현한 liblinear 라이브러리를 기반으로 함

     - liblinear 라이브러리
       - 커널 트릭을 지원하지 않음
       - but 훈련샘플과 특성 수에 거의 선형적으로 늘어남
       - 알고리즘의 훈련시간 복잡도 : O(m*n)

   - 정밀도를 높이면 알고리즘의 수행시간이 길어짐

     - 허용오차 하이퍼파라미터로 조절 (사이킷런에서는 매개변수 tol)

   - SVC는 커널 트릭 알고리즘을 구현한 libsvm 라이브러리를 기반으로 함

     - 훈련 시간복잡도 : O(m^2 * n) 과 O(m^3 * n) 사이
     - 훈련 샘플 수가 커지면 엄청나게 느려짐
     - 복잡하지만 작거나 중간 규모의 훈련세트에 적합
     - 특성의 개수에서 (특히 희소특성) 잘 확장됨
       - 알고리즘의 성능이 샘플이 가진 0이 아닌 특성의 평균 수에 거의 비례함

   - ![image](https://user-images.githubusercontent.com/97875918/188306599-1dfb2ade-69a3-4442-ad4a-f2480c111dd5.png)

     













# SVM 회귀

- SVM을 분류가 아니라 회귀에 적용하는 방법

  - 목표를 반대로 하는 것

    - SVM 분류 : 마진 위반 발생 정도를 조절하면서 두 클래스 사이의 도로폭을 최대한 넓게 하기
    - SVM 회귀 : 마진 위반 발생 정도를 조절하면서 도로폭을 최대한 넓혀서 도로 위에 가능한 많은 샘플 포함하기
      - 회귀 모델의 마진 위반 사례 : 도로 밖에 위치한 샘플

  - 도로의 폭은 하이퍼파라미터로 조절

  - ![image](https://user-images.githubusercontent.com/97875918/188314436-175f2981-efb2-43f9-b19d-4d0dd510b4d1.png)

    - SVM회귀
    - 왼쪽 그래프
      - 마진을 크게
      - 입실론에 민감하지 않다
    - 오른쪽 그래프
      - 마진을 작게
    - SVM 회귀모델인 SVR과 LinearSVR에서 허용오차는 tol, 도로의 폭은 epsilon 매개변수로 지정

  - 마진 안에서는 훈련 샘플이 추가되어도 모델 예측에는 영향x

  - ![image](https://user-images.githubusercontent.com/97875918/188314668-e80aed39-2418-4075-9651-2dcdd547ebc0.png)

    - 2차 다항 커널을 사용한 SVM회귀
    - 왼쪽은 아주 큰 C로 규제가 거의 없는 그래프
      - 도로폭이 더 넓음

  - SVR

    - SVC의 회귀 버전
    - 훈련세트가 커지면 훨씬 느려짐 (=SVC)

  - LinearSVR

    - LinearSVC의 회귀 버전
    - 필요한 시간이 훈련세트의 크기에 비례해 선형적으로 늘어남 (=LinearSVC)

    









